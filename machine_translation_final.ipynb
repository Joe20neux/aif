{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traduction de texte \n",
    "### Oribes Lucas, Payet Marine, Piedra Tristan, Quetin Sebastien, Testi Joevin\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Preprocessing** - Dans un premier temps pour passer des données textuelles à un réseau de neuronnes, nous devons convertir le texte en entier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%aimport helper, tests\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "import project_tests as tests\n",
    "import tensorflow \n",
    "\n",
    "\n",
    "from keras import utils as ut\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, Dropout, LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from keras.losses import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Dans un premier temps on essaye de créer des réseaux qui fonctionnent sur de petites phrases. On essayera dans un second temps des traductions plus importantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded\n"
     ]
    }
   ],
   "source": [
    "english_sentences = helper.load_data('data/small_vocab_en')\n",
    "french_sentences = helper.load_data('data/small_vocab_fr')\n",
    "print('Dataset Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le dataset french sentences est la traduction du dataset french sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sample 1:  new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "French sample 1:  new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "\n",
      "English sample 2:  the united states is usually chilly during july , and it is usually freezing in november .\n",
      "French sample 2:  les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n",
      "\n",
      "English sample 3:  california is usually quiet during march , and it is usually hot in june .\n",
      "French sample 3:  california est généralement calme en mars , et il est généralement chaud en juin .\n",
      "\n",
      "English sample 4:  the united states is sometimes mild during june , and it is cold in september .\n",
      "French sample 4:  les états-unis est parfois légère en juin , et il fait froid en septembre .\n",
      "\n",
      "English sample 5:  your least liked fruit is the grape , but my least liked is the apple .\n",
      "French sample 5:  votre moins aimé fruit est le raisin , mais mon moins aimé est la pomme .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sample_i in range(5):\n",
    "    print('English sample {}:  {}'.format(sample_i + 1, english_sentences[sample_i]))\n",
    "    print('French sample {}:  {}\\n'.format(sample_i + 1, french_sentences[sample_i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On étudie la complexité du jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1823250 English words.\n",
      "227 unique English words.\n",
      "10 Most common words in the English dataset:\n",
      "\"is\" \",\" \".\" \"in\" \"it\" \"during\" \"the\" \"but\" \"and\" \"sometimes\"\n",
      "\n",
      "1961295 French words.\n",
      "355 unique French words.\n",
      "10 Most common words in the French dataset:\n",
      "\"est\" \".\" \",\" \"en\" \"il\" \"les\" \"mais\" \"et\" \"la\" \"parfois\"\n"
     ]
    }
   ],
   "source": [
    "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
    "french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n",
    "\n",
    "print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n",
    "print('{} unique English words.'.format(len(english_words_counter)))\n",
    "print('10 Most common words in the English dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n",
    "print()\n",
    "print('{} French words.'.format(len([word for sentence in french_sentences for word in sentence.split()])))\n",
    "print('{} unique French words.'.format(len(french_words_counter)))\n",
    "print('10 Most common words in the French dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Preprocessing\n",
    "Pour pouvoir donner nos entrées au réseau de neuronnes on a besoin de transformer nos données textuelles en numériques. Pour cela on va \"tokenizer\" nos données c'est  à dire leur attribuer un nombre dans la base de mots, soit francais, soit anglais. Cela permettra au réseau d'identifier les mots.\n",
    "Ensuite, comme les phrases francaises ont une taille de 21 mots maximum et les phrases anglaises 15 mots, on va rajouter des \"0\" à la fin des phrases anglaises. C'est ce que l'on appelle: le padding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1, 'quick': 2, 'a': 3, 'brown': 4, 'fox': 5, 'jumps': 6, 'over': 7, 'lazy': 8, 'dog': 9, 'by': 10, 'jove': 11, 'my': 12, 'study': 13, 'of': 14, 'lexicography': 15, 'won': 16, 'prize': 17, 'this': 18, 'is': 19, 'short': 20, 'sentence': 21}\n",
      "\n",
      "Sequence 1 in x\n",
      "  Input:  The quick brown fox jumps over the lazy dog .\n",
      "  Output: [1, 2, 4, 5, 6, 7, 1, 8, 9]\n",
      "Sequence 2 in x\n",
      "  Input:  By Jove , my quick study of lexicography won a prize .\n",
      "  Output: [10, 11, 12, 2, 13, 14, 15, 16, 3, 17]\n",
      "Sequence 3 in x\n",
      "  Input:  This is a short sentence .\n",
      "  Output: [18, 19, 3, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(x):\n",
    "    \"\"\"\n",
    "    Tokenize x\n",
    "    :param x: List of sentences/strings to be tokenized\n",
    "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    return tokenizer.texts_to_sequences(x), tokenizer\n",
    "\n",
    "tests.test_tokenize(tokenize)\n",
    "\n",
    "# Tokenize Example output\n",
    "text_sentences = [\n",
    "    'The quick brown fox jumps over the lazy dog .',\n",
    "    'By Jove , my quick study of lexicography won a prize .',\n",
    "    'This is a short sentence .']\n",
    "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
    "print(text_tokenizer.word_index)\n",
    "print()\n",
    "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(sent))\n",
    "    print('  Output: {}'.format(token_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1 in x\n",
      "  Input:  [1 2 4 5 6 7 1 8 9]\n",
      "  Output: [1 2 4 5 6 7 1 8 9 0]\n",
      "Sequence 2 in x\n",
      "  Input:  [10 11 12  2 13 14 15 16  3 17]\n",
      "  Output: [10 11 12  2 13 14 15 16  3 17]\n",
      "Sequence 3 in x\n",
      "  Input:  [18 19  3 20 21]\n",
      "  Output: [18 19  3 20 21  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "def pad(x, length=None):\n",
    "    \"\"\"\n",
    "    Pad x\n",
    "    :param x: List of sequences.\n",
    "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
    "    :return: Padded numpy array of sequences\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    return pad_sequences(x, maxlen=length, padding='post')\n",
    "\n",
    "tests.test_pad(pad)\n",
    "\n",
    "# Pad Tokenized output\n",
    "test_pad = pad(text_tokenized)\n",
    "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(np.array(token_sent)))\n",
    "    print('  Output: {}'.format(pad_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed\n",
      "Max English sentence length: 15\n",
      "Max French sentence length: 21\n",
      "English vocabulary size: 199\n",
      "French vocabulary size: 344\n"
     ]
    }
   ],
   "source": [
    "def preprocess(x, y):\n",
    "    \"\"\"\n",
    "    Preprocess x and y\n",
    "    :param x: Feature List of sentences\n",
    "    :param y: Label List of sentences\n",
    "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
    "    \"\"\"\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "\n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "\n",
    "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n",
    "    preprocess(english_sentences, french_sentences)\n",
    "    \n",
    "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
    "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
    "english_vocab_size = len(english_tokenizer.word_index)\n",
    "french_vocab_size = len(french_tokenizer.word_index)\n",
    "\n",
    "print('Data Preprocessed')\n",
    "print(\"Max English sentence length:\", max_english_sequence_length)\n",
    "print(\"Max French sentence length:\", max_french_sequence_length)\n",
    "print(\"English vocabulary size:\", english_vocab_size)\n",
    "print(\"French vocabulary size:\", french_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin il nous faut une fonction qui permet une fois que le réseau nous sort un ID de mot dans une base, de retrouver ce mot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`logits_to_text` function loaded.\n"
     ]
    }
   ],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "    \"\"\"\n",
    "    Turn logits from a neural network into text using the tokenizer\n",
    "    :param logits: Logits from a neural network\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "\n",
    "print('`logits_to_text` function loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# evaluate the skill of the model\n",
    "def evaluate_model(model,  sources, raw_dataset,pred):\n",
    "    actual, predicted = list(), list()\n",
    "\n",
    "    for i in range(pred.shape[0]):\n",
    "        \n",
    "        # translate encoded source text\n",
    "        translation = logits_to_text(pred[i], french_tokenizer)\n",
    "        #raw_target=raw_dataset[i][0]\n",
    "        raw_target = logits_to_text(raw_dataset[i], french_tokenizer)\n",
    "        if i < 10:\n",
    "            print(' target=[%s], predicted=[%s]' % ( raw_target, translation))\n",
    "        actual.append([raw_target.split()])\n",
    "        predicted.append(translation.split())\n",
    "        \n",
    "    # calculate BLEU score\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons tester plusieurs réseaux de neuronnes. UN réseau simple, un encoder-decoder, un bidirectionnel, un mélange de ces réseaux. Toutes ces méthodes consistent en un \"one hot encoding\" de tous les mots avant de les passer au réseau de neuronnes. Nous allons ensuite simplement garder la liste des 'ID' de mots dans la phrase pour la passer à des réseaux qui fonctionnent avec une couche embedding qui permet de vectoriser les mots. Enfin on essayera une méthode qui consiste à combiner toutes ces méthodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: RNN (IMPLEMENTATION)\n",
    "![RNN](images/rnn.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping the input to work with a basic RNN\n",
    "tmp_x = pad(preproc_english_sentences, max_french_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
    "\n",
    "preproc_french_sentencesbis=ut.to_categorical(preproc_french_sentences)\n",
    "                                              \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_embed, X_test_embed, y_train_embed, y_test_embed = train_test_split(tmp_x, preproc_french_sentencesbis, test_size=0.33, random_state=42)\n",
    "X_train = ut.to_categorical(X_train_embed)\n",
    "X_test = ut.to_categorical(X_test_embed) \n",
    "y_train = y_train_embed\n",
    "y_test = y_test_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/insa/anaconda/envs/GPU/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/insa/anaconda/envs/GPU/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_1 (GRU)                  (None, 21, 256)           350976    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 21, 1024)          263168    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 21, 1024)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 21, 345)           353625    \n",
      "=================================================================\n",
      "Total params: 967,769\n",
      "Trainable params: 967,769\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /usr/local/insa/anaconda/envs/GPU/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 73892 samples, validate on 18474 samples\n",
      "Epoch 1/30\n",
      "73892/73892 [==============================] - 63s 856us/step - loss: 3.1124 - acc: 0.4563 - val_loss: 2.0506 - val_acc: 0.5842\n",
      "Epoch 2/30\n",
      "73892/73892 [==============================] - 6s 82us/step - loss: 1.6369 - acc: 0.6145 - val_loss: 1.2413 - val_acc: 0.6679\n",
      "Epoch 3/30\n",
      "73892/73892 [==============================] - 6s 82us/step - loss: 1.0710 - acc: 0.7054 - val_loss: 0.8461 - val_acc: 0.7559\n",
      "Epoch 4/30\n",
      "73892/73892 [==============================] - 6s 81us/step - loss: 0.7988 - acc: 0.7617 - val_loss: 0.6722 - val_acc: 0.7958\n",
      "Epoch 5/30\n",
      "73892/73892 [==============================] - 6s 83us/step - loss: 0.6578 - acc: 0.7947 - val_loss: 0.5651 - val_acc: 0.8189\n",
      "Epoch 6/30\n",
      "73892/73892 [==============================] - 6s 83us/step - loss: 0.5666 - acc: 0.8176 - val_loss: 0.5238 - val_acc: 0.8256\n",
      "Epoch 7/30\n",
      "73892/73892 [==============================] - 6s 83us/step - loss: 0.5092 - acc: 0.8319 - val_loss: 0.4403 - val_acc: 0.8552\n",
      "Epoch 8/30\n",
      "73892/73892 [==============================] - 6s 83us/step - loss: 0.4569 - acc: 0.8476 - val_loss: 0.4190 - val_acc: 0.8602\n",
      "Epoch 9/30\n",
      "73892/73892 [==============================] - 6s 83us/step - loss: 0.4183 - acc: 0.8601 - val_loss: 0.3865 - val_acc: 0.8684\n",
      "Epoch 10/30\n",
      "73892/73892 [==============================] - 6s 82us/step - loss: 0.3931 - acc: 0.8675 - val_loss: 0.3510 - val_acc: 0.8808\n",
      "Epoch 11/30\n",
      "73892/73892 [==============================] - 6s 82us/step - loss: 0.3626 - acc: 0.8777 - val_loss: 0.3275 - val_acc: 0.8883\n",
      "Epoch 12/30\n",
      "73892/73892 [==============================] - 6s 83us/step - loss: 0.3394 - acc: 0.8858 - val_loss: 0.3098 - val_acc: 0.8947\n",
      "Epoch 13/30\n",
      "73892/73892 [==============================] - 6s 83us/step - loss: 0.3186 - acc: 0.8928 - val_loss: 0.3005 - val_acc: 0.8990\n",
      "Epoch 14/30\n",
      "73892/73892 [==============================] - 6s 83us/step - loss: 0.3084 - acc: 0.8958 - val_loss: 0.2874 - val_acc: 0.9015\n",
      "Epoch 15/30\n",
      "73892/73892 [==============================] - 6s 83us/step - loss: 0.2933 - acc: 0.9007 - val_loss: 0.2699 - val_acc: 0.9083\n",
      "Epoch 16/30\n",
      "73892/73892 [==============================] - 6s 82us/step - loss: 0.2858 - acc: 0.9033 - val_loss: 0.2625 - val_acc: 0.9103\n",
      "Epoch 17/30\n",
      "73892/73892 [==============================] - 6s 82us/step - loss: 0.2698 - acc: 0.9085 - val_loss: 0.2561 - val_acc: 0.9128\n",
      "Epoch 18/30\n",
      "73892/73892 [==============================] - 6s 82us/step - loss: 0.2588 - acc: 0.9121 - val_loss: 0.2434 - val_acc: 0.9165\n",
      "Epoch 19/30\n",
      "73892/73892 [==============================] - 6s 82us/step - loss: 0.2506 - acc: 0.9146 - val_loss: 0.2423 - val_acc: 0.9167\n",
      "Epoch 20/30\n",
      "73892/73892 [==============================] - 6s 83us/step - loss: 0.2425 - acc: 0.9170 - val_loss: 0.2326 - val_acc: 0.9204\n",
      "Epoch 21/30\n",
      "73892/73892 [==============================] - 6s 83us/step - loss: 0.2371 - acc: 0.9188 - val_loss: 0.2278 - val_acc: 0.9220\n",
      "Epoch 22/30\n",
      "73892/73892 [==============================] - 6s 82us/step - loss: 0.2310 - acc: 0.9206 - val_loss: 0.2196 - val_acc: 0.9240\n",
      "Epoch 23/30\n",
      "73892/73892 [==============================] - 6s 83us/step - loss: 0.2248 - acc: 0.9226 - val_loss: 0.2202 - val_acc: 0.9234\n",
      "Epoch 24/30\n",
      "73892/73892 [==============================] - 6s 82us/step - loss: 0.2218 - acc: 0.9232 - val_loss: 0.2148 - val_acc: 0.9259\n",
      "Epoch 25/30\n",
      "73892/73892 [==============================] - 6s 82us/step - loss: 0.2160 - acc: 0.9251 - val_loss: 0.2107 - val_acc: 0.9267\n",
      "Epoch 26/30\n",
      "73892/73892 [==============================] - 6s 83us/step - loss: 0.2115 - acc: 0.9266 - val_loss: 0.2072 - val_acc: 0.9280\n",
      "Epoch 27/30\n",
      "73892/73892 [==============================] - 6s 82us/step - loss: 0.2089 - acc: 0.9271 - val_loss: 0.2067 - val_acc: 0.9283\n",
      "Epoch 28/30\n",
      "73892/73892 [==============================] - 6s 83us/step - loss: 0.2062 - acc: 0.9281 - val_loss: 0.2029 - val_acc: 0.9293\n",
      "Epoch 29/30\n",
      "73892/73892 [==============================] - 6s 82us/step - loss: 0.2005 - acc: 0.9296 - val_loss: 0.2014 - val_acc: 0.9298\n",
      "Epoch 30/30\n",
      "73892/73892 [==============================] - 6s 82us/step - loss: 0.1972 - acc: 0.9307 - val_loss: 0.1965 - val_acc: 0.9314\n",
      "californie est parfois sec en mois de mai et il est parfois merveilleux en février <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "def simple_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a basic RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.005\n",
    "    \n",
    "    # TODO: Build the layers\n",
    "    model = Sequential()\n",
    "    model.add(GRU(256, input_shape=input_shape[1:], return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size+1, activation='softmax'))) \n",
    "    \n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss=categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#tests.test_simple_model(simple_model)\n",
    "\n",
    "\n",
    "\n",
    "# Train the neural network\n",
    "simple_rnn_model = simple_model(\n",
    "    X_train.shape,\n",
    "    max_french_sequence_length,\n",
    "    english_vocab_size,\n",
    "    french_vocab_size)\n",
    "\n",
    "print(simple_rnn_model.summary())\n",
    "\n",
    "simple_rnn_model.fit(X_train,y_train, batch_size = 5000, epochs=30, validation_split=0.2)\n",
    "\n",
    "\n",
    "# Print prediction(s)\n",
    "print(logits_to_text(simple_rnn_model.predict(X_test)[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=simple_rnn_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION : californie est parfois sec en mois de mai et il est parfois merveilleux en février <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : californie est parfois sec au mois de mai et il est parfois merveilleux en février <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : californie est jamais chaud en l'automne mais mais il est sec sec mars mars <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : californie est jamais chaud pendant l' automne mais il est jamais sec en mars <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : les états unis est parfois pluvieux en janvier mais il est doux en mai <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : les états unis est parfois pluvieux en janvier mais il est doux en mai <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : ils aiment les pommes les oranges et les poires <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : ils aiment les pommes les oranges et les poires <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : il n'aime pas mangues et et fraises fraises <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : il n'aime les mangues et les fraises <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : les pamplemousse est son fruit préféré mais la citron est votre favori <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : le pamplemousse est son fruit préféré mais le citron est votre favori <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : l' inde est parfois agréable au cours de l' automne mais il est généralement agréable en mars <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : l' inde est parfois agréable au cours de l' automne mais il est généralement agréable en mars <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : votre fruit le plus aimé est la pomme mais son plus aimé est la pêche <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : votre fruit le plus aimé est la pomme mais son plus aimé est la pêche <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : les états unis est froid en l' hiver et il est relaxant relaxant en juin <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : les états unis est froid pendant l' hiver et il est parfois relaxant en juin <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : je les le pamplemousse pommes pommes et les mangues <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : i comme le pamplemousse les pommes et les mangues <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : votre fruit aimé fruit est la le pamplemousse mais notre moins aimé est la mangue <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : votre moins aimé des fruits est le pamplemousse mais notre moins aimé est la mangue <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : ils aiment les fraises les citrons et les poires <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : ils aiment les fraises les citrons et les poires <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : new jersey est généralement beau en mars et il est chaud en hiver <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : new jersey est généralement beau en mars et il est chaud en hiver <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : les poire est son fruit préféré mais la citron est leur favori <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : la poire est son fruit préféré mais le citron est leur favori <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : les requin est son animal plus aimé aimé <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : le requin est son animal le plus aimé <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : la france est jamais relaxant en juillet mais il est chaud en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : la france est jamais relaxant en juillet mais il est chaud en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : chine est est parfois chaud décembre mais il est jamais en octobre <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : chine est parfois chaude en décembre mais il gèle jamais en novembre <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : new jersey est occupé en juillet et il est relaxant en juin <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : new jersey est occupé en juillet et il est relaxant en juin <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : mon fruit est moins aimé la mais son moins aimé est la pomme <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : mon fruit est moins aimé l'orange mais son moins aimé est la banane <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : chine est est parfois occupé en juillet mais il est généralement chaud en août <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : la chine est parfois occupé en juillet mais il est généralement chaud en août <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(len(predictions[:20])):\n",
    "    print('PREDICTION :',logits_to_text(predictions[i], french_tokenizer))\n",
    "    print('ATTENDU :',logits_to_text(y_test[i], french_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      " target=[les états unis est jamais belle au mois de novembre et il est jamais tranquille en juillet <PAD> <PAD> <PAD> <PAD>], predicted=[les états unis est jamais beau en mois de novembre et il est jamais tranquille en juillet <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[le singe est votre animal préféré moins <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[les singe est votre animal préféré moins <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[californie ne fait jamais froid pendant l' hiver mais il est jamais tranquille à l' automne <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[californie est jamais jamais froid pendant l' hiver mais il est jamais tranquille à l' automne <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[la pêche est leur fruit le plus cher mais le citron est le plus aimé <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[les pêche est leur fruit le plus cher mais le citron est le plus aimé <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[vos animaux les plus aimés étaient des chevaux <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[votre fruit le plus aimés étaient des chevaux <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[l'orange est votre fruit préféré moins mais la fraise est son moins préféré <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[les est son fruit préféré moins mais la chaux est son moins préféré <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[les états unis est généralement agréable en juillet mais il est sec à l' automne <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[les états unis est généralement agréable en juillet mais il est sec en l' automne <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[elle a conduit une brillante voiture noire <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[elle a conduit une voiture voiture noire <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[l' inde est parfois clémentes en septembre et il est parfois humide en hiver <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[l' inde est parfois doux en septembre et il est parfois humide en hiver <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[paris est beau au mois d' août mais il est agréable au printemps <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[paris est beau au mois d' août mais il est agréable au printemps <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      "BLEU-1: 0.950762\n",
      "BLEU-2: 0.928939\n",
      "BLEU-3: 0.917677\n",
      "BLEU-4: 0.890061\n",
      "TEST\n",
      " target=[californie est parfois sec au mois de mai et il est parfois merveilleux en février <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[californie est parfois sec en mois de mai et il est parfois merveilleux en février <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[californie est jamais chaud pendant l' automne mais il est jamais sec en mars <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[californie est jamais chaud en l'automne mais mais il est sec sec mars mars <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[les états unis est parfois pluvieux en janvier mais il est doux en mai <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[les états unis est parfois pluvieux en janvier mais il est doux en mai <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[ils aiment les pommes les oranges et les poires <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[ils aiment les pommes les oranges et les poires <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[il n'aime les mangues et les fraises <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[il n'aime pas mangues et et fraises fraises <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[le pamplemousse est son fruit préféré mais le citron est votre favori <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[les pamplemousse est son fruit préféré mais la citron est votre favori <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[l' inde est parfois agréable au cours de l' automne mais il est généralement agréable en mars <PAD> <PAD> <PAD> <PAD>], predicted=[l' inde est parfois agréable au cours de l' automne mais il est généralement agréable en mars <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[votre fruit le plus aimé est la pomme mais son plus aimé est la pêche <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[votre fruit le plus aimé est la pomme mais son plus aimé est la pêche <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[les états unis est froid pendant l' hiver et il est parfois relaxant en juin <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[les états unis est froid en l' hiver et il est relaxant relaxant en juin <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[i comme le pamplemousse les pommes et les mangues <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[je les le pamplemousse pommes pommes et les mangues <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      "BLEU-1: 0.949264\n",
      "BLEU-2: 0.927049\n",
      "BLEU-3: 0.915526\n",
      "BLEU-4: 0.887246\n"
     ]
    }
   ],
   "source": [
    "# test on some training sequences\n",
    "print('TRAIN')\n",
    "pred1=simple_rnn_model.predict(X_train)\n",
    "evaluate_model(simple_rnn_model , X_train, y_train,pred1)\n",
    "# test on some test sequences\n",
    "print('TEST')\n",
    "evaluate_model(simple_rnn_model, X_test, y_test,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Bidirectional RNNs (IMPLEMENTATION)\n",
    "![RNN](images/bidirectional.png)\n",
    "\n",
    "Un réseau RNN ne peut pas voir le futur de ce qu'on lui passe en entrée, il peut seulement voir son passé. C'est pour pallier à cette limite que nous utilisons les bidirectional recurrent neural networks. Ils sont capables de voir le futur de nos données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 21, 256)           252672    \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 21, 1024)          263168    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 21, 1024)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 21, 345)           353625    \n",
      "=================================================================\n",
      "Total params: 869,465\n",
      "Trainable params: 869,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 73892 samples, validate on 18474 samples\n",
      "Epoch 1/10\n",
      "73892/73892 [==============================] - 8s 107us/step - loss: 1.8045 - acc: 0.6175 - val_loss: 0.7981 - val_acc: 0.7722\n",
      "Epoch 2/10\n",
      "73892/73892 [==============================] - 8s 104us/step - loss: 0.6390 - acc: 0.8061 - val_loss: 0.4648 - val_acc: 0.8503\n",
      "Epoch 3/10\n",
      "73892/73892 [==============================] - 8s 103us/step - loss: 0.4285 - acc: 0.8611 - val_loss: 0.3432 - val_acc: 0.8855\n",
      "Epoch 4/10\n",
      "73892/73892 [==============================] - 8s 104us/step - loss: 0.3359 - acc: 0.8888 - val_loss: 0.2827 - val_acc: 0.9064\n",
      "Epoch 5/10\n",
      "73892/73892 [==============================] - 8s 104us/step - loss: 0.2804 - acc: 0.9068 - val_loss: 0.2521 - val_acc: 0.9142\n",
      "Epoch 6/10\n",
      "73892/73892 [==============================] - 8s 104us/step - loss: 0.2438 - acc: 0.9191 - val_loss: 0.2061 - val_acc: 0.9327\n",
      "Epoch 7/10\n",
      "73892/73892 [==============================] - 8s 104us/step - loss: 0.2113 - acc: 0.9304 - val_loss: 0.1839 - val_acc: 0.9394\n",
      "Epoch 8/10\n",
      "73892/73892 [==============================] - 8s 102us/step - loss: 0.1909 - acc: 0.9369 - val_loss: 0.1653 - val_acc: 0.9455\n",
      "Epoch 9/10\n",
      "73892/73892 [==============================] - 8s 105us/step - loss: 0.1715 - acc: 0.9440 - val_loss: 0.1484 - val_acc: 0.9520\n",
      "Epoch 10/10\n",
      "73892/73892 [==============================] - 8s 103us/step - loss: 0.1559 - acc: 0.9492 - val_loss: 0.1350 - val_acc: 0.9560\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbc60eaadd8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bd_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a bidirectional RNN model on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.003\n",
    "    \n",
    "    # TODO: Build the layers\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(GRU(128, return_sequences=True), input_shape=input_shape[1:]))\n",
    "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax'))) \n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss=categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#tests.test_bd_model(bd_model)\n",
    "\n",
    "# TODO: Reshape the input\n",
    "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2]))\n",
    "\n",
    "\n",
    "preproc_french_sentencesbis=ut.to_categorical(preproc_french_sentences)\n",
    "tmp_xbis=ut.to_categorical(tmp_x)\n",
    "# TODO: Train and Print prediction(s)\n",
    "bde_model = bd_model(\n",
    "    X_train.shape,\n",
    "    preproc_french_sentencesbis.shape[1],\n",
    "    len(english_tokenizer.word_index)+1,\n",
    "    len(french_tokenizer.word_index)+1)\n",
    "\n",
    "bde_model.summary()\n",
    "\n",
    "bde_model.fit(X_train, y_train, batch_size=1024, epochs=10, validation_split=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsBDE=bde_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION : californie est parfois sec au mois de mai et il est parfois merveilleux en février <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : californie est parfois sec au mois de mai et il est parfois merveilleux en février <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : californie est jamais chaud à l' mais il est jamais sec en mars <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : californie est jamais chaud pendant l' automne mais il est jamais sec en mars <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : les états unis est parfois pluvieux en janvier mais il est doux en mai <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : les états unis est parfois pluvieux en janvier mais il est doux en mai <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : ils aiment les pommes les oranges et les poires <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : ils aiment les pommes les oranges et les poires <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : il n'aime les mangues et les les fraises <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : il n'aime les mangues et les fraises <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : le pamplemousse est son fruit préféré mais le citron est votre favori <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : le pamplemousse est son fruit préféré mais le citron est votre favori <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : l' inde est parfois agréable au cours de l' automne mais il est agréable agréable en <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : l' inde est parfois agréable au cours de l' automne mais il est généralement agréable en mars <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : votre fruit le plus aimé est la pomme mais son plus aimé est la pêche <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : votre fruit le plus aimé est la pomme mais son plus aimé est la pêche <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : les états unis est froid pendant l' hiver et il est parfois relaxant en juin <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : les états unis est froid pendant l' hiver et il est parfois relaxant en juin <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : i comme le pamplemousse pommes et et les mangues <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : i comme le pamplemousse les pommes et les mangues <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : votre moins aimé des fruits est le pamplemousse mais notre moins aimé est la mangue <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : votre moins aimé des fruits est le pamplemousse mais notre moins aimé est la mangue <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : ils aiment les fraises les citrons et les poires <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : ils aiment les fraises les citrons et les poires <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : new jersey est généralement beau en mars et il est chaud en hiver <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : new jersey est généralement beau en mars et il est chaud en hiver <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : la poire est son fruit préféré mais le citron est leur favori <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : la poire est son fruit préféré mais le citron est leur favori <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : le requin est son animal le plus aimé <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : le requin est son animal le plus aimé <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : la france est jamais relaxant en juillet mais il est chaud en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : la france est jamais relaxant en juillet mais il est chaud en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : chine est parfois chaude en décembre mais il gèle jamais en novembre <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : chine est parfois chaude en décembre mais il gèle jamais en novembre <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : new jersey est occupé en juillet et il est relaxant en juin <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : new jersey est occupé en juillet et il est relaxant en juin <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : mon moins est moins aimé l'orange mais son moins aimé est la banane <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : mon fruit est moins aimé l'orange mais son moins aimé est la banane <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : la chine est parfois en juillet mais mais il est généralement en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : la chine est parfois occupé en juillet mais il est généralement chaud en août <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(predictionsBDE[:20])):\n",
    "    print('PREDICTION :',logits_to_text(predictionsBDE[i], french_tokenizer))\n",
    "    print('ATTENDU :',logits_to_text(y_test[i], french_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      " target=[les états unis est jamais belle au mois de novembre et il est jamais tranquille en juillet <PAD> <PAD> <PAD> <PAD>], predicted=[les états unis est jamais belle en mois de novembre et il est jamais tranquille en juillet <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[le singe est votre animal préféré moins <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[le singe est votre animal préféré moins <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[californie ne fait jamais froid pendant l' hiver mais il est jamais tranquille à l' automne <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[californie ne fait jamais froid pendant l' hiver mais il est jamais tranquille à l' automne <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[la pêche est leur fruit le plus cher mais le citron est le plus aimé <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[la pêche est leur fruit le plus cher mais le citron est le plus aimé <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[vos animaux les plus aimés étaient des chevaux <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[vos animaux les plus aimés étaient des chevaux <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[l'orange est votre fruit préféré moins mais la fraise est son moins préféré <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[l'orange est votre fruit préféré moins mais la fraise est son moins préféré <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[les états unis est généralement agréable en juillet mais il est sec à l' automne <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[les états unis est généralement agréable en juillet mais il est sec à l' automne <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[elle a conduit une brillante voiture noire <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[elle a conduit une voiture voiture noire <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[l' inde est parfois clémentes en septembre et il est parfois humide en hiver <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[l' inde est parfois clémentes en septembre et il est parfois humide en hiver <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[paris est beau au mois d' août mais il est agréable au printemps <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[paris est beau au mois d' août mais il est agréable au printemps <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      "BLEU-1: 0.978807\n",
      "BLEU-2: 0.968495\n",
      "BLEU-3: 0.962408\n",
      "BLEU-4: 0.948650\n",
      "TEST\n",
      " target=[californie est parfois sec au mois de mai et il est parfois merveilleux en février <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[californie est parfois sec au mois de mai et il est parfois merveilleux en février <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[californie est jamais chaud pendant l' automne mais il est jamais sec en mars <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[californie est jamais chaud à l' mais il est jamais sec en mars <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[les états unis est parfois pluvieux en janvier mais il est doux en mai <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[les états unis est parfois pluvieux en janvier mais il est doux en mai <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[ils aiment les pommes les oranges et les poires <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[ils aiment les pommes les oranges et les poires <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[il n'aime les mangues et les fraises <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[il n'aime les mangues et les les fraises <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[le pamplemousse est son fruit préféré mais le citron est votre favori <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[le pamplemousse est son fruit préféré mais le citron est votre favori <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[l' inde est parfois agréable au cours de l' automne mais il est généralement agréable en mars <PAD> <PAD> <PAD> <PAD>], predicted=[l' inde est parfois agréable au cours de l' automne mais il est agréable agréable en <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[votre fruit le plus aimé est la pomme mais son plus aimé est la pêche <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[votre fruit le plus aimé est la pomme mais son plus aimé est la pêche <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[les états unis est froid pendant l' hiver et il est parfois relaxant en juin <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[les états unis est froid pendant l' hiver et il est parfois relaxant en juin <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[i comme le pamplemousse les pommes et les mangues <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[i comme le pamplemousse pommes et et les mangues <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      "BLEU-1: 0.978040\n",
      "BLEU-2: 0.967330\n",
      "BLEU-3: 0.961032\n",
      "BLEU-4: 0.946818\n"
     ]
    }
   ],
   "source": [
    "# test on some training sequences\n",
    "print('TRAIN')\n",
    "pred1=bde_model.predict(X_train)\n",
    "evaluate_model(bde_model , X_train, y_train,pred1)\n",
    "# test on some test sequences\n",
    "print('TEST')\n",
    "evaluate_model(bde_model, X_test, y_test,predictionsBDE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Encoder-Decoder\n",
    "\n",
    "Regardons à présent les modèles encoder-decocer. Ce modèle est composé d'un encoder et d'un decoder. L'encoder crée une matrice qui représente les phrases. Le décodeur prend cette matrice comme entrée et prédit les translations en sortie. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_3 (GRU)                  (None, 256)               350976    \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, 21, 256)           393984    \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 21, 1024)          263168    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 21, 1024)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 21, 345)           353625    \n",
      "=================================================================\n",
      "Total params: 1,361,753\n",
      "Trainable params: 1,361,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 73892 samples, validate on 18474 samples\n",
      "Epoch 1/10\n",
      "73892/73892 [==============================] - 10s 138us/step - loss: 3.1586 - acc: 0.4203 - val_loss: 2.3771 - val_acc: 0.4610\n",
      "Epoch 2/10\n",
      "73892/73892 [==============================] - 9s 116us/step - loss: 2.0840 - acc: 0.5020 - val_loss: 1.8227 - val_acc: 0.5377\n",
      "Epoch 3/10\n",
      "73892/73892 [==============================] - 9s 120us/step - loss: 1.7228 - acc: 0.5565 - val_loss: 1.5290 - val_acc: 0.5967\n",
      "Epoch 4/10\n",
      "73892/73892 [==============================] - 9s 119us/step - loss: 1.4824 - acc: 0.6089 - val_loss: 1.3196 - val_acc: 0.6450\n",
      "Epoch 5/10\n",
      "73892/73892 [==============================] - 9s 120us/step - loss: 1.3358 - acc: 0.6416 - val_loss: 1.2059 - val_acc: 0.6668\n",
      "Epoch 6/10\n",
      "73892/73892 [==============================] - 9s 120us/step - loss: 1.2167 - acc: 0.6651 - val_loss: 1.1155 - val_acc: 0.6844\n",
      "Epoch 7/10\n",
      "73892/73892 [==============================] - 9s 119us/step - loss: 1.1300 - acc: 0.6836 - val_loss: 1.0602 - val_acc: 0.7007\n",
      "Epoch 8/10\n",
      "73892/73892 [==============================] - 9s 120us/step - loss: 1.0543 - acc: 0.7011 - val_loss: 0.9758 - val_acc: 0.7233\n",
      "Epoch 9/10\n",
      "73892/73892 [==============================] - 9s 121us/step - loss: 1.0059 - acc: 0.7122 - val_loss: 0.9101 - val_acc: 0.7380\n",
      "Epoch 10/10\n",
      "73892/73892 [==============================] - 9s 121us/step - loss: 0.9389 - acc: 0.7287 - val_loss: 0.8805 - val_acc: 0.7469\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbc5ef90d68>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encdec_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train an encoder-decoder model on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    \n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    # Build the layers    \n",
    "    model = Sequential()\n",
    "    # Encoder\n",
    "    model.add(GRU(256, input_shape=input_shape[1:], go_backwards=True))\n",
    "    model.add(RepeatVector(output_sequence_length))\n",
    "    # Decoder\n",
    "    model.add(GRU(256, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss=categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "#tests.test_encdec_model(encdec_model)\n",
    "\n",
    "# Reshape the input\n",
    "#tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "#tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
    "\n",
    "\n",
    "#preproc_french_sentencesbis=ut.to_categorical(preproc_french_sentences)\n",
    "#tmp_xbis=ut.to_categorical(tmp_x)\n",
    "# Train and Print prediction(s)\n",
    "encdec_rnn_model = encdec_model(\n",
    "    X_train.shape,\n",
    "    preproc_french_sentencesbis.shape[1],\n",
    "    len(english_tokenizer.word_index)+1,\n",
    "    len(french_tokenizer.word_index)+1)\n",
    "\n",
    "encdec_rnn_model.summary()\n",
    "\n",
    "encdec_rnn_model.fit(X_train, y_train, batch_size=1024, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsencdec=encdec_rnn_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION : californie est parfois parfois au mois de mai et il est parfois en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : californie est parfois sec au mois de mai et il est parfois merveilleux en février <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : california est jamais froid au l' de il il est parfois en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : californie est jamais chaud pendant l' automne mais il est jamais sec en mars <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : les états unis est parfois froid en avril et il est en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : les états unis est parfois pluvieux en janvier mais il est doux en mai <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : ils aiment les les les les et les <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : ils aiment les pommes les oranges et les poires <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : il aime les les et les citrons <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : il n'aime les mangues et les fraises <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : le raisin est son fruit préféré mais la chaux est son préféré <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : le pamplemousse est son fruit préféré mais le citron est votre favori <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : l' inde est parfois agréable au l'automne et il est est parfois en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : l' inde est parfois agréable au cours de l' automne mais il est généralement agréable en mars <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : votre fruit le plus aimé est la chaux mais son plus aimé est la <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : votre fruit le plus aimé est la pomme mais son plus aimé est la pêche <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : les états unis est froid pendant l' hiver et il est est en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : les états unis est froid pendant l' hiver et il est parfois relaxant en juin <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : j'aime les pamplemousse pamplemousse et les les <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : i comme le pamplemousse les pommes et les mangues <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : votre moins aimé aimé aimé la la mais son moins aimé est la poire <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : votre moins aimé des fruits est le pamplemousse mais notre moins aimé est la mangue <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : ils aiment les les les et les les <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : ils aiment les fraises les citrons et les poires <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : new jersey est généralement agréable en printemps et il est en en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : new jersey est généralement beau en mars et il est chaud en hiver <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : la poire est son fruit préféré mais la est est son préféré <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : la poire est son fruit préféré mais le citron est leur favori <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : le pamplemousse était votre animal animal plus aimé <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : le requin est son animal le plus aimé <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : la france est jamais relaxant en avril et il est en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : la france est jamais relaxant en juillet mais il est chaud en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : chine est parfois parfois en avril et il est jamais en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : chine est parfois chaude en décembre mais il gèle jamais en novembre <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : new jersey est froid en avril et il est en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : new jersey est occupé en juillet et il est relaxant en juin <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : mon fruit aimé aimé est la mais mais moins aimé est la <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : mon fruit est moins aimé l'orange mais son moins aimé est la banane <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : chine est parfois parfois en avril et il est parfois en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : la chine est parfois occupé en juillet mais il est généralement chaud en août <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(predictionsencdec[:20])):\n",
    "    print('PREDICTION :',logits_to_text(predictionsencdec[i], french_tokenizer))\n",
    "    print('ATTENDU :',logits_to_text(y_test[i], french_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      " target=[les états unis est jamais belle au mois de novembre et il est jamais tranquille en juillet <PAD> <PAD> <PAD> <PAD>], predicted=[les états unis est jamais froid en mois et il est est jamais en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[le singe est votre animal préféré moins <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[le est était animal animal préféré <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[californie ne fait jamais froid pendant l' hiver mais il est jamais tranquille à l' automne <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[california est jamais parfois froid pendant l' mai il il est parfois en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[la pêche est leur fruit le plus cher mais le citron est le plus aimé <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[la poire est leur fruit le plus cher mais la est est plus plus aimé <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[vos animaux les plus aimés étaient des chevaux <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[ses animaux animaux le redoutés des les <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[l'orange est votre fruit préféré moins mais la fraise est son moins préféré <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[l'orange est votre fruit préféré moins mais la est est son moins préféré <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[les états unis est généralement agréable en juillet mais il est sec à l' automne <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[les états unis est généralement agréable en avril et il est agréable en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[elle a conduit une brillante voiture noire <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[elle a conduit une voiture voiture <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[l' inde est parfois clémentes en septembre et il est parfois humide en hiver <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[l' inde est parfois chaud en septembre et il est parfois en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[paris est beau au mois d' août mais il est agréable au printemps <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[paris est agréable au mois d' août et il est en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      "BLEU-1: 0.788679\n",
      "BLEU-2: 0.697089\n",
      "BLEU-3: 0.650984\n",
      "BLEU-4: 0.554559\n",
      "TEST\n",
      " target=[californie est parfois sec au mois de mai et il est parfois merveilleux en février <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[californie est parfois parfois au mois de mai et il est parfois en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[californie est jamais chaud pendant l' automne mais il est jamais sec en mars <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[california est jamais froid au l' de il il est parfois en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[les états unis est parfois pluvieux en janvier mais il est doux en mai <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[les états unis est parfois froid en avril et il est en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[ils aiment les pommes les oranges et les poires <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[ils aiment les les les les et les <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[il n'aime les mangues et les fraises <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[il aime les les et les citrons <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[le pamplemousse est son fruit préféré mais le citron est votre favori <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[le raisin est son fruit préféré mais la chaux est son préféré <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[l' inde est parfois agréable au cours de l' automne mais il est généralement agréable en mars <PAD> <PAD> <PAD> <PAD>], predicted=[l' inde est parfois agréable au l'automne et il est est parfois en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[votre fruit le plus aimé est la pomme mais son plus aimé est la pêche <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[votre fruit le plus aimé est la chaux mais son plus aimé est la <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[les états unis est froid pendant l' hiver et il est parfois relaxant en juin <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[les états unis est froid pendant l' hiver et il est est en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[i comme le pamplemousse les pommes et les mangues <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[j'aime les pamplemousse pamplemousse et les les <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      "BLEU-1: 0.788589\n",
      "BLEU-2: 0.696859\n",
      "BLEU-3: 0.650710\n",
      "BLEU-4: 0.554308\n"
     ]
    }
   ],
   "source": [
    "# test on some training sequences\n",
    "print('TRAIN')\n",
    "pred1=encdec_rnn_model.predict(X_train)\n",
    "evaluate_model(encdec_rnn_model , X_train, y_train,pred1)\n",
    "# test on some test sequences\n",
    "print('TEST')\n",
    "evaluate_model(encdec_rnn_model, X_test, y_test,predictionsencdec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: Custom (IMPLEMENTATION)\n",
    "\n",
    "\n",
    "Nous allons maintenant créer un modèle qui comprend à la fois de l'embedding et un bidirectionnel rnn dans un même modèle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_2 (Bidirection (None, 256)               252672    \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 21, 256)           295680    \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, 21, 512)           131584    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 21, 512)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 21, 345)           176985    \n",
      "=================================================================\n",
      "Total params: 856,921\n",
      "Trainable params: 856,921\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 73892 samples, validate on 18474 samples\n",
      "Epoch 1/25\n",
      "73892/73892 [==============================] - 12s 164us/step - loss: 2.8333 - acc: 0.4287 - val_loss: 1.8682 - val_acc: 0.5387\n",
      "Epoch 2/25\n",
      "73892/73892 [==============================] - 10s 137us/step - loss: 1.6821 - acc: 0.5729 - val_loss: 1.4110 - val_acc: 0.6251\n",
      "Epoch 3/25\n",
      "73892/73892 [==============================] - 11s 143us/step - loss: 1.3655 - acc: 0.6373 - val_loss: 1.2281 - val_acc: 0.6601\n",
      "Epoch 4/25\n",
      "73892/73892 [==============================] - 10s 131us/step - loss: 1.1917 - acc: 0.6709 - val_loss: 1.0328 - val_acc: 0.7109\n",
      "Epoch 5/25\n",
      "73892/73892 [==============================] - 10s 131us/step - loss: 1.0521 - acc: 0.7025 - val_loss: 0.9931 - val_acc: 0.7171\n",
      "Epoch 6/25\n",
      "73892/73892 [==============================] - 10s 131us/step - loss: 0.9831 - acc: 0.7179 - val_loss: 0.8537 - val_acc: 0.7525\n",
      "Epoch 7/25\n",
      "73892/73892 [==============================] - 10s 132us/step - loss: 0.8856 - acc: 0.7407 - val_loss: 0.7692 - val_acc: 0.7682\n",
      "Epoch 8/25\n",
      "73892/73892 [==============================] - 10s 131us/step - loss: 0.7980 - acc: 0.7609 - val_loss: 0.7052 - val_acc: 0.7848\n",
      "Epoch 9/25\n",
      "73892/73892 [==============================] - 10s 133us/step - loss: 0.7455 - acc: 0.7731 - val_loss: 0.6567 - val_acc: 0.8000\n",
      "Epoch 10/25\n",
      "73892/73892 [==============================] - 10s 133us/step - loss: 0.6750 - acc: 0.7921 - val_loss: 0.5836 - val_acc: 0.8220\n",
      "Epoch 11/25\n",
      "73892/73892 [==============================] - 10s 132us/step - loss: 0.6254 - acc: 0.8052 - val_loss: 0.5324 - val_acc: 0.8346\n",
      "Epoch 12/25\n",
      "73892/73892 [==============================] - 10s 133us/step - loss: 0.5859 - acc: 0.8158 - val_loss: 0.4855 - val_acc: 0.8470\n",
      "Epoch 13/25\n",
      "73892/73892 [==============================] - 10s 131us/step - loss: 0.5537 - acc: 0.8249 - val_loss: 0.5885 - val_acc: 0.8152\n",
      "Epoch 14/25\n",
      "73892/73892 [==============================] - 10s 132us/step - loss: 0.5103 - acc: 0.8382 - val_loss: 0.4118 - val_acc: 0.8703\n",
      "Epoch 15/25\n",
      "73892/73892 [==============================] - 10s 132us/step - loss: 0.4446 - acc: 0.8591 - val_loss: 0.3770 - val_acc: 0.8833\n",
      "Epoch 16/25\n",
      "73892/73892 [==============================] - 10s 132us/step - loss: 0.4124 - acc: 0.8694 - val_loss: 0.3538 - val_acc: 0.8885\n",
      "Epoch 17/25\n",
      "73892/73892 [==============================] - 10s 133us/step - loss: 0.3698 - acc: 0.8827 - val_loss: 0.3058 - val_acc: 0.9051\n",
      "Epoch 18/25\n",
      "73892/73892 [==============================] - 10s 132us/step - loss: 0.3418 - acc: 0.8908 - val_loss: 0.2670 - val_acc: 0.9169\n",
      "Epoch 19/25\n",
      "73892/73892 [==============================] - 10s 133us/step - loss: 0.3244 - acc: 0.8956 - val_loss: 0.2444 - val_acc: 0.9232\n",
      "Epoch 20/25\n",
      "73892/73892 [==============================] - 10s 133us/step - loss: 0.2902 - acc: 0.9067 - val_loss: 0.2449 - val_acc: 0.9242\n",
      "Epoch 21/25\n",
      "73892/73892 [==============================] - 10s 131us/step - loss: 0.2705 - acc: 0.9136 - val_loss: 0.1994 - val_acc: 0.9392\n",
      "Epoch 22/25\n",
      "73892/73892 [==============================] - 10s 130us/step - loss: 0.2596 - acc: 0.9176 - val_loss: 0.2041 - val_acc: 0.9408\n",
      "Epoch 23/25\n",
      "73892/73892 [==============================] - 10s 132us/step - loss: 0.2328 - acc: 0.9290 - val_loss: 0.1650 - val_acc: 0.9540\n",
      "Epoch 24/25\n",
      "73892/73892 [==============================] - 10s 130us/step - loss: 0.2006 - acc: 0.9406 - val_loss: 0.1468 - val_acc: 0.9582\n",
      "Epoch 25/25\n",
      "73892/73892 [==============================] - 10s 134us/step - loss: 0.1850 - acc: 0.9453 - val_loss: 0.2340 - val_acc: 0.9402\n",
      "Final Model Loaded\n"
     ]
    }
   ],
   "source": [
    "def model_final(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a model that incorporates embedding, encoder-decoder, and bidirectional RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.003\n",
    "    \n",
    "    # Build the layers    \n",
    "    model = Sequential()\n",
    "    # Embedding\n",
    "    #model.add(Embedding(english_vocab_size, 128, input_length=input_shape[1],\n",
    "                         #input_shape=input_shape[1:]))\n",
    "    \n",
    "    # Encoder\n",
    "    model.add(Bidirectional(GRU(128),input_shape=input_shape[1:]))\n",
    "    model.add(RepeatVector(output_sequence_length))\n",
    "    # Decoder\n",
    "    model.add(Bidirectional(GRU(128, return_sequences=True)))\n",
    "    model.add(TimeDistributed(Dense(512, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
    "    model.compile(loss=categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "model_melange=model_final(X_train.shape,y_train.shape[1],\n",
    "                        len(english_tokenizer.word_index)+1,\n",
    "                        len(french_tokenizer.word_index)+1)\n",
    "model_melange.summary()\n",
    "model_melange.fit(X_train, y_train, batch_size=1024, epochs=25, validation_split=0.2)\n",
    "\n",
    "\n",
    "#tests.test_model_final(model_final)\n",
    "\n",
    "print('Final Model Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_melange=model_melange.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION : californie est parfois sec au mois de mai et il est parfois merveilleux en février <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : californie est parfois sec au mois de mai et il est parfois merveilleux en février <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : californie est jamais chaud pendant l' automne mais il est jamais sec en mars <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : californie est jamais chaud pendant l' automne mais il est jamais sec en mars <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : les états unis est parfois pluvieux en janvier mais il est doux en mai <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : les états unis est parfois pluvieux en janvier mais il est doux en mai <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : ils aiment les pommes les oranges et les poires <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : ils aiment les pommes les oranges et les poires <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : il n'aime les mangues et les fraises <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : il n'aime les mangues et les fraises <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : le pamplemousse est son fruit préféré mais le citron est votre favori <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : le pamplemousse est son fruit préféré mais le citron est votre favori <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : l' inde est parfois agréable au cours de l' automne mais il il est agréable en <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : l' inde est parfois agréable au cours de l' automne mais il est généralement agréable en mars <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : votre fruit le plus aimé est la pomme mais son plus aimé est la pêche <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : votre fruit le plus aimé est la pomme mais son plus aimé est la pêche <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : les états unis est froid pendant l' hiver et il est parfois relaxant en juin <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : les états unis est froid pendant l' hiver et il est parfois relaxant en juin <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : i comme pamplemousse pamplemousse les et et les mangues <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : i comme le pamplemousse les pommes et les mangues <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : votre moins aimé des fruits est le pamplemousse mais notre moins aimé est la mangue <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : votre moins aimé des fruits est le pamplemousse mais notre moins aimé est la mangue <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : ils aiment les fraises les citrons et les poires <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : ils aiment les fraises les citrons et les poires <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : new jersey est généralement beau en mars et il est chaud en hiver <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : new jersey est généralement beau en mars et il est chaud en hiver <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : la poire est son fruit préféré mais le citron est leur favori <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : la poire est son fruit préféré mais le citron est leur favori <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : le requin est son animal le plus aimé <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : le requin est son animal le plus aimé <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : la france est jamais relaxant en juillet mais il est chaud en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : la france est jamais relaxant en juillet mais il est chaud en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : chine est parfois chaud en décembre mais il est jamais en en novembre <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : chine est parfois chaude en décembre mais il gèle jamais en novembre <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : new jersey est occupé en juillet et il est relaxant en juin <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : new jersey est occupé en juillet et il est relaxant en juin <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : mon fruit est moins aimé l'orange mais son moins aimé est la banane <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : mon fruit est moins aimé l'orange mais son moins aimé est la banane <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : la chine est parfois occupé en juillet mais il est généralement chaud en août <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : la chine est parfois occupé en juillet mais il est généralement chaud en août <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(predictions_melange[:20])):\n",
    "    print('PREDICTION :',logits_to_text(predictions_melange[i], french_tokenizer))\n",
    "    print('ATTENDU :',logits_to_text(y_test[i], french_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      " target=[les états unis est jamais belle au mois de novembre et il est jamais tranquille en juillet <PAD> <PAD> <PAD> <PAD>], predicted=[les états unis est jamais belle au mois de novembre et il est jamais jamais en en <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[le singe est votre animal préféré moins <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[le singe est votre animal préféré moins <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[californie ne fait jamais froid pendant l' hiver mais il est jamais tranquille à l' automne <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[californie ne fait jamais froid pendant l' hiver mais il est jamais tranquille à l' automne <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[la pêche est leur fruit le plus cher mais le citron est le plus aimé <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[la pêche est leur fruit le plus cher mais le citron est son plus aimé <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[vos animaux les plus aimés étaient des chevaux <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[vos animaux les plus aimés étaient des chevaux <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[l'orange est votre fruit préféré moins mais la fraise est son moins préféré <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[l'orange est votre fruit préféré moins mais la fraise est son moins préféré <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[les états unis est généralement agréable en juillet mais il est sec à l' automne <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[les états unis est généralement agréable en juillet mais mais il est sec à l' automne <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[elle a conduit une brillante voiture noire <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[elle a conduit une une voiture noire <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[l' inde est parfois clémentes en septembre et il est parfois humide en hiver <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[l' inde est parfois clémentes en septembre et il est parfois humide en hiver <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[paris est beau au mois d' août mais il est agréable au printemps <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[paris est beau au mois d' août mais il est agréable au printemps <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      "BLEU-1: 0.973084\n",
      "BLEU-2: 0.961069\n",
      "BLEU-3: 0.953582\n",
      "BLEU-4: 0.936425\n",
      "TEST\n",
      " target=[californie est parfois sec au mois de mai et il est parfois merveilleux en février <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[californie est parfois sec au mois de mai et il est parfois merveilleux en février <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[californie est jamais chaud pendant l' automne mais il est jamais sec en mars <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[californie est jamais chaud pendant l' automne mais il est jamais sec en mars <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[les états unis est parfois pluvieux en janvier mais il est doux en mai <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[les états unis est parfois pluvieux en janvier mais il est doux en mai <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[ils aiment les pommes les oranges et les poires <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[ils aiment les pommes les oranges et les poires <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[il n'aime les mangues et les fraises <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[il n'aime les mangues et les fraises <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[le pamplemousse est son fruit préféré mais le citron est votre favori <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[le pamplemousse est son fruit préféré mais le citron est votre favori <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[l' inde est parfois agréable au cours de l' automne mais il est généralement agréable en mars <PAD> <PAD> <PAD> <PAD>], predicted=[l' inde est parfois agréable au cours de l' automne mais il il est agréable en <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[votre fruit le plus aimé est la pomme mais son plus aimé est la pêche <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[votre fruit le plus aimé est la pomme mais son plus aimé est la pêche <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[les états unis est froid pendant l' hiver et il est parfois relaxant en juin <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[les états unis est froid pendant l' hiver et il est parfois relaxant en juin <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[i comme le pamplemousse les pommes et les mangues <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[i comme pamplemousse pamplemousse les et et les mangues <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      "BLEU-1: 0.972044\n",
      "BLEU-2: 0.959590\n",
      "BLEU-3: 0.951886\n",
      "BLEU-4: 0.934197\n"
     ]
    }
   ],
   "source": [
    "# test on some training sequences\n",
    "print('TRAIN')\n",
    "pred1=model_melange.predict(X_train)\n",
    "evaluate_model(model_melange , X_train, y_train,pred1)\n",
    "# test on some test sequences\n",
    "print('TEST')\n",
    "evaluate_model(model_melange, X_test, y_test,predictions_melange)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèle 5: Embedding (IMPLEMENTATION)\n",
    "![RNN](images/embedding.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce modèle on tente de transformer les mots en vecteurs dont les mots proches en sens sont proches en vecteurs. On prendra ici en entrée X_train_embed que nous n'vons pas \"one-hot\" encodé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_embed=X_train_embed.reshape((X_train_embed.shape[0],X_train_embed.shape[1]))\n",
    "X_test_embed=X_test_embed.reshape((X_test_embed.shape[0],X_test_embed.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45495, 21)\n",
      "(92366, 21)\n"
     ]
    }
   ],
   "source": [
    "print(X_test_embed.shape)\n",
    "print(X_train_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45495, 21, 345)\n",
      "(92366, 21, 345)\n"
     ]
    }
   ],
   "source": [
    "print(y_test_embed.shape)\n",
    "print(y_train_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 21, 128)           25600     \n",
      "_________________________________________________________________\n",
      "gru_7 (GRU)                  (None, 21, 256)           295680    \n",
      "_________________________________________________________________\n",
      "time_distributed_9 (TimeDist (None, 21, 512)           131584    \n",
      "_________________________________________________________________\n",
      "gru_8 (GRU)                  (None, 21, 256)           590592    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_10 (TimeDis (None, 21, 345)           88665     \n",
      "=================================================================\n",
      "Total params: 1,132,121\n",
      "Trainable params: 1,132,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 73892 samples, validate on 18474 samples\n",
      "Epoch 1/10\n",
      "73892/73892 [==============================] - 9s 118us/step - loss: 2.1955 - acc: 0.5414 - val_loss: 1.0897 - val_acc: 0.7124\n",
      "Epoch 2/10\n",
      "73892/73892 [==============================] - 8s 106us/step - loss: 0.8574 - acc: 0.7628 - val_loss: 0.5690 - val_acc: 0.8277\n",
      "Epoch 3/10\n",
      "73892/73892 [==============================] - 8s 106us/step - loss: 0.5688 - acc: 0.8304 - val_loss: 0.4523 - val_acc: 0.8597\n",
      "Epoch 4/10\n",
      "73892/73892 [==============================] - 8s 105us/step - loss: 0.4537 - acc: 0.8640 - val_loss: 0.3536 - val_acc: 0.8882\n",
      "Epoch 5/10\n",
      "73892/73892 [==============================] - 8s 105us/step - loss: 0.3828 - acc: 0.8856 - val_loss: 0.3031 - val_acc: 0.9040\n",
      "Epoch 6/10\n",
      "73892/73892 [==============================] - 8s 106us/step - loss: 0.3396 - acc: 0.8980 - val_loss: 0.2727 - val_acc: 0.9134\n",
      "Epoch 7/10\n",
      "73892/73892 [==============================] - 8s 106us/step - loss: 0.3096 - acc: 0.9061 - val_loss: 0.2811 - val_acc: 0.9120\n",
      "Epoch 8/10\n",
      "73892/73892 [==============================] - 8s 105us/step - loss: 0.2945 - acc: 0.9107 - val_loss: 0.2399 - val_acc: 0.9228\n",
      "Epoch 9/10\n",
      "73892/73892 [==============================] - 8s 107us/step - loss: 0.2701 - acc: 0.9171 - val_loss: 0.2327 - val_acc: 0.9256\n",
      "Epoch 10/10\n",
      "73892/73892 [==============================] - 8s 104us/step - loss: 0.2651 - acc: 0.9182 - val_loss: 0.2272 - val_acc: 0.9265\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbb81a26d30>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def embed_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps,input_shape):\n",
    "    \"\"\"\n",
    "    Build and train a RNN model using word embedding on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(src_vocab,output_dim=128, mask_zero=False,input_shape=input_shape[1:]))\n",
    "    #mask_zero doit etre a false sinon il ne reconnaitra jamais les pad et mettra des mots a la place\n",
    "    model.add(GRU(256,return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(512, activation='relu')))\n",
    "    model.add(GRU(256,return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size+1, activation='softmax')))\n",
    "\n",
    "    return model\n",
    "learning_rate = 0.005\n",
    "\n",
    "\n",
    "embed_rnn_model = embed_model(\n",
    "    len(english_tokenizer.word_index)+1,\n",
    "    len(french_tokenizer.word_index)+1,\n",
    "    y_train_embed.shape[1:],\n",
    "    21,\n",
    "    X_train_embed.shape)\n",
    "\n",
    "embed_rnn_model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "embed_rnn_model.summary()\n",
    "\n",
    "embed_rnn_model.fit(X_train_embed, y_train_embed, batch_size=1024, epochs=10, validation_split=0.2)\n",
    "\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION : californie est parfois sec en mois de mai et il est parfois merveilleux en février <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : californie est parfois sec au mois de mai et il est parfois merveilleux en février <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : californie est jamais chaud en l'automne mais il est jamais jamais en mars <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : californie est jamais chaud pendant l' automne mais il est jamais sec en mars <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : les états unis est parfois pluvieux en janvier mais il est doux en mai <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : les états unis est parfois pluvieux en janvier mais il est doux en mai <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : ils aiment les pommes les oranges et les poires <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : ils aiment les pommes les oranges et les poires <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : il aime pas mangues et les les fraises <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : il n'aime les mangues et les fraises <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : les pamplemousse est son fruit préféré mais la citron est votre favori <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : le pamplemousse est son fruit préféré mais le citron est votre favori <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : l' inde est parfois agréable en cours de il est généralement agréable en généralement agréable en <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : l' inde est parfois agréable au cours de l' automne mais il est généralement agréable en mars <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : votre fruit le plus aimé est la pomme mais son plus aimé est la pêche <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : votre fruit le plus aimé est la pomme mais son plus aimé est la pêche <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : les états unis est froid en l' hiver et il est parfois relaxant en juin <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : les états unis est froid pendant l' hiver et il est parfois relaxant en juin <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : je les le pamplemousse les pommes et les mangues <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : i comme le pamplemousse les pommes et les mangues <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : votre moins aimé fruit est la le pamplemousse mais notre moins aimé est la mangue <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : votre moins aimé des fruits est le pamplemousse mais notre moins aimé est la mangue <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : ils aiment les fraises les citrons et les poires <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : ils aiment les fraises les citrons et les poires <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : new jersey est généralement beau en mars et il est chaud en hiver <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : new jersey est généralement beau en mars et il est chaud en hiver <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : les poire est son fruit préféré mais la citron est leur favori <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : la poire est son fruit préféré mais le citron est leur favori <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : les requin est son animal le plus aimé <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : le requin est son animal le plus aimé <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : la france est jamais relaxant en juillet mais il est chaud en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : la france est jamais relaxant en juillet mais il est chaud en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : chine est est chaud chaud décembre mais il est jamais en février <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : chine est parfois chaude en décembre mais il gèle jamais en novembre <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : new jersey est occupé en juillet et il est relaxant en juin <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : new jersey est occupé en juillet et il est relaxant en juin <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : mon fruit est moins aimé la mais son moins aimé est la mangue <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : mon fruit est moins aimé l'orange mais son moins aimé est la banane <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : chine est est occupée en juillet mais il est généralement chaud en août <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : la chine est parfois occupé en juillet mais il est généralement chaud en août <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "# TODO: Print prediction(s)\n",
    "pred_embed=embed_rnn_model.predict(X_test_embed)\n",
    "for i in range(len(pred_embed[:20])):\n",
    "    print('PREDICTION :',logits_to_text(pred_embed[i], french_tokenizer))\n",
    "    print('ATTENDU :',logits_to_text(y_test[i], french_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      " target=[les états unis est jamais belle au mois de novembre et il est jamais tranquille en juillet <PAD> <PAD> <PAD> <PAD>], predicted=[les états unis est jamais beau en mois de novembre et il est jamais tranquille en juillet <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[le singe est votre animal préféré moins <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[les singe est votre animal préféré moins <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[californie ne fait jamais froid pendant l' hiver mais il est jamais tranquille à l' automne <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[californie est jamais jamais froid pendant l' hiver mais il est jamais tranquille à l' automne <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[la pêche est leur fruit le plus cher mais le citron est le plus aimé <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[les pêche est leur fruit le plus cher mais le citron est le plus aimé <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[vos animaux les plus aimés étaient des chevaux <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[votre fruit le plus aimés étaient des chevaux <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[l'orange est votre fruit préféré moins mais la fraise est son moins préféré <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[les est son fruit moins moins mais la citron est son moins préféré <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[les états unis est généralement agréable en juillet mais il est sec à l' automne <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[les états unis est généralement agréable en juillet mais il est sec en l' automne <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[elle a conduit une brillante voiture noire <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[elle a conduit une voiture voiture noire <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[l' inde est parfois clémentes en septembre et il est parfois humide en hiver <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[l' inde est parfois doux en septembre et il est parfois humide en hiver <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[paris est beau au mois d' août mais il est agréable au printemps <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[paris est beau en mois d' août mais il est agréable au printemps <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      "BLEU-1: 0.948257\n",
      "BLEU-2: 0.925369\n",
      "BLEU-3: 0.913701\n",
      "BLEU-4: 0.884883\n",
      "TEST\n",
      " target=[californie est parfois sec au mois de mai et il est parfois merveilleux en février <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[californie est parfois sec en mois de mai et il est parfois merveilleux en février <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[californie est jamais chaud pendant l' automne mais il est jamais sec en mars <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[californie est jamais chaud en l'automne mais il est jamais jamais en mars <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[les états unis est parfois pluvieux en janvier mais il est doux en mai <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[les états unis est parfois pluvieux en janvier mais il est doux en mai <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[ils aiment les pommes les oranges et les poires <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[ils aiment les pommes les oranges et les poires <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[il n'aime les mangues et les fraises <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[il aime pas mangues et les les fraises <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[le pamplemousse est son fruit préféré mais le citron est votre favori <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[les pamplemousse est son fruit préféré mais la citron est votre favori <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[l' inde est parfois agréable au cours de l' automne mais il est généralement agréable en mars <PAD> <PAD> <PAD> <PAD>], predicted=[l' inde est parfois agréable en cours de il est généralement agréable en généralement agréable en <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[votre fruit le plus aimé est la pomme mais son plus aimé est la pêche <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[votre fruit le plus aimé est la pomme mais son plus aimé est la pêche <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[les états unis est froid pendant l' hiver et il est parfois relaxant en juin <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[les états unis est froid en l' hiver et il est parfois relaxant en juin <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[i comme le pamplemousse les pommes et les mangues <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[je les le pamplemousse les pommes et les mangues <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      "BLEU-1: 0.946887\n",
      "BLEU-2: 0.923599\n",
      "BLEU-3: 0.911731\n",
      "BLEU-4: 0.882351\n"
     ]
    }
   ],
   "source": [
    "# test on some training sequences\n",
    "print('TRAIN')\n",
    "pred1=embed_rnn_model.predict(X_train_embed)\n",
    "evaluate_model(embed_rnn_model , X_train_embed, y_train_embed,pred1)\n",
    "# test on some test sequences\n",
    "print('TEST')\n",
    "evaluate_model(embed_rnn_model, X_test_embed, y_test_embed,pred_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MEGA COMBO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce modèle est un mélange des trois types de modèles précédement utilisés: bidirectionel,embedding et encoder-decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 21, 15)            3000      \n",
      "_________________________________________________________________\n",
      "gru_9 (GRU)                  (None, 21, 256)           208896    \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 256)               295680    \n",
      "_________________________________________________________________\n",
      "repeat_vector_3 (RepeatVecto (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 21, 256)           295680    \n",
      "_________________________________________________________________\n",
      "time_distributed_11 (TimeDis (None, 21, 512)           131584    \n",
      "_________________________________________________________________\n",
      "gru_12 (GRU)                 (None, 21, 256)           590592    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_12 (TimeDis (None, 21, 345)           88665     \n",
      "=================================================================\n",
      "Total params: 1,614,097\n",
      "Trainable params: 1,614,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 73892 samples, validate on 18474 samples\n",
      "Epoch 1/20\n",
      "73892/73892 [==============================] - 17s 231us/step - loss: 2.9821 - acc: 0.4354 - val_loss: 2.1555 - val_acc: 0.5044\n",
      "Epoch 2/20\n",
      "73892/73892 [==============================] - 13s 176us/step - loss: 1.8273 - acc: 0.5517 - val_loss: 1.4137 - val_acc: 0.6141\n",
      "Epoch 3/20\n",
      "73892/73892 [==============================] - 14s 189us/step - loss: 1.3341 - acc: 0.6357 - val_loss: 1.1925 - val_acc: 0.6659\n",
      "Epoch 4/20\n",
      "73892/73892 [==============================] - 13s 179us/step - loss: 1.1236 - acc: 0.6818 - val_loss: 0.9744 - val_acc: 0.7112\n",
      "Epoch 5/20\n",
      "73892/73892 [==============================] - 13s 176us/step - loss: 1.0236 - acc: 0.7043 - val_loss: 1.0189 - val_acc: 0.7021\n",
      "Epoch 6/20\n",
      "73892/73892 [==============================] - 13s 175us/step - loss: 0.9184 - acc: 0.7254 - val_loss: 0.7855 - val_acc: 0.7549\n",
      "Epoch 7/20\n",
      "73892/73892 [==============================] - 13s 182us/step - loss: 0.8206 - acc: 0.7501 - val_loss: 0.7138 - val_acc: 0.7742\n",
      "Epoch 8/20\n",
      "73892/73892 [==============================] - 14s 186us/step - loss: 0.7468 - acc: 0.7702 - val_loss: 0.6283 - val_acc: 0.8005\n",
      "Epoch 9/20\n",
      "73892/73892 [==============================] - 13s 180us/step - loss: 0.6555 - acc: 0.7959 - val_loss: 0.5984 - val_acc: 0.8121\n",
      "Epoch 10/20\n",
      "73892/73892 [==============================] - 13s 176us/step - loss: 0.5958 - acc: 0.8139 - val_loss: 0.5630 - val_acc: 0.8190\n",
      "Epoch 11/20\n",
      "73892/73892 [==============================] - 13s 175us/step - loss: 0.5510 - acc: 0.8274 - val_loss: 0.4407 - val_acc: 0.8583\n",
      "Epoch 12/20\n",
      "73892/73892 [==============================] - 13s 182us/step - loss: 0.5004 - acc: 0.8430 - val_loss: 0.4946 - val_acc: 0.8414\n",
      "Epoch 13/20\n",
      "73892/73892 [==============================] - 14s 183us/step - loss: 0.4415 - acc: 0.8599 - val_loss: 0.3264 - val_acc: 0.8933\n",
      "Epoch 14/20\n",
      "73892/73892 [==============================] - 14s 186us/step - loss: 0.3793 - acc: 0.8800 - val_loss: 0.3013 - val_acc: 0.9026\n",
      "Epoch 15/20\n",
      "73892/73892 [==============================] - 13s 181us/step - loss: 0.3374 - acc: 0.8935 - val_loss: 0.3003 - val_acc: 0.9058\n",
      "Epoch 16/20\n",
      "73892/73892 [==============================] - 13s 175us/step - loss: 0.3029 - acc: 0.9055 - val_loss: 0.2333 - val_acc: 0.9275\n",
      "Epoch 17/20\n",
      "73892/73892 [==============================] - 13s 179us/step - loss: 0.2684 - acc: 0.9183 - val_loss: 0.2131 - val_acc: 0.9374\n",
      "Epoch 18/20\n",
      "73892/73892 [==============================] - 14s 183us/step - loss: 0.2420 - acc: 0.9292 - val_loss: 0.1805 - val_acc: 0.9473\n",
      "Epoch 19/20\n",
      "73892/73892 [==============================] - 13s 178us/step - loss: 0.2015 - acc: 0.9426 - val_loss: 0.1876 - val_acc: 0.9476\n",
      "Epoch 20/20\n",
      "73892/73892 [==============================] - 13s 175us/step - loss: 0.2580 - acc: 0.9270 - val_loss: 0.2024 - val_acc: 0.9426\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbb8026e5f8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mega_combo_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps,input_shape):\n",
    "    \"\"\"\n",
    "    Build and train a RNN model using word embedding on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(src_vocab,15, mask_zero=False, input_shape=input_shape[1:]))\n",
    "    model.add(GRU(256,return_sequences=True))\n",
    "    model.add(Bidirectional(GRU(128)))\n",
    "    model.add(RepeatVector(21))\n",
    "    model.add(Bidirectional(GRU(128, return_sequences=True)))\n",
    "    model.add(TimeDistributed(Dense(512, activation='relu')))\n",
    "    model.add(GRU(256,return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size+1, activation='softmax')))\n",
    "\n",
    "    return model\n",
    "learning_rate = 0.005\n",
    "\n",
    "\n",
    "MEGA_model = mega_combo_model(\n",
    "    len(english_tokenizer.word_index)+1,\n",
    "    len(french_tokenizer.word_index)+1,\n",
    "    y_train.shape[1:],\n",
    "    21,\n",
    "    X_train_embed.shape)\n",
    "\n",
    "MEGA_model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "MEGA_model.summary()\n",
    "\n",
    "MEGA_model.fit(X_train_embed, y_train_embed, batch_size=1024, epochs=20, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mega_pred=MEGA_model.predict(X_test_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION : californie est parfois doux au mois de mai et il est parfois merveilleux en février <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : californie est parfois sec au mois de mai et il est parfois merveilleux en février <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : california est jamais chaud à l'automne mais il est jamais jamais sec mars <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : californie est jamais chaud pendant l' automne mais il est jamais sec en mars <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : les états unis est parfois pluvieux en janvier mais il est doux en mai <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : les états unis est parfois pluvieux en janvier mais il est doux en mai <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : ils aiment les pommes les oranges et les poires <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : ils aiment les pommes les oranges et les poires <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : il aime pas mangues mangues et les <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : il n'aime les mangues et les fraises <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : le pamplemousse est son fruit préféré mais le citron est votre favori <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : le pamplemousse est son fruit préféré mais le citron est votre favori <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : l' inde est parfois agréable au cours de l' automne mais il est généralement agréable en mars <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : l' inde est parfois agréable au cours de l' automne mais il est généralement agréable en mars <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : votre fruit le plus aimé est la pomme mais son plus aimé est la pêche <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : votre fruit le plus aimé est la pomme mais son plus aimé est la pêche <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : les états unis est froid pendant l' hiver et il est parfois parfois en juin <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : les états unis est froid pendant l' hiver et il est parfois relaxant en juin <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : i comme le pamplemousse les et et les mangues <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : i comme le pamplemousse les pommes et les mangues <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : votre moins aimé des fruits est le pamplemousse mais notre moins aimé est la mangue <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : votre moins aimé des fruits est le pamplemousse mais notre moins aimé est la mangue <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : ils aiment les fraises les citrons et les poires <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : ils aiment les fraises les citrons et les poires <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : new jersey est généralement beau en mars et il est chaud en hiver <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : new jersey est généralement beau en mars et il est chaud en hiver <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : la poire est son fruit préféré mais le citron est leur favori <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : la poire est son fruit préféré mais le citron est leur favori <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : le requin est son animal le plus aimé <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : le requin est son animal le plus aimé <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : la france est jamais relaxant en juillet mais il est chaud en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : la france est jamais relaxant en juillet mais il est chaud en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : la est est parfois en décembre mais mais il gèle jamais en novembre <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : chine est parfois chaude en décembre mais il gèle jamais en novembre <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : new jersey est occupé en juillet et il est relaxant en juin <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : new jersey est occupé en juillet et il est relaxant en juin <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : mon fruit est moins aimé l'orange mais son moins aimé est la banane <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : mon fruit est moins aimé l'orange mais son moins aimé est la banane <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : chine chine est parfois occupé en juillet mais il est généralement chaud en août <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : la chine est parfois occupé en juillet mais il est généralement chaud en août <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(mega_pred[:20])):\n",
    "    print('PREDICTION :',logits_to_text(mega_pred[i], french_tokenizer))\n",
    "    print('ATTENDU :',logits_to_text(y_test[i], french_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      " target=[les états unis est jamais belle au mois de novembre et il est jamais tranquille en juillet <PAD> <PAD> <PAD> <PAD>], predicted=[les états unis est jamais beau au mois de novembre et il est jamais tranquille en juillet <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[le singe est votre animal préféré moins <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[le singe est votre animal préféré moins <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[californie ne fait jamais froid pendant l' hiver mais il est jamais tranquille à l' automne <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[californie ne fait jamais froid pendant l' hiver mais il est jamais tranquille à l' automne <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[la pêche est leur fruit le plus cher mais le citron est le plus aimé <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[la pêche est leur fruit le plus cher mais le citron est le plus aimé <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[vos animaux les plus aimés étaient des chevaux <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[vos animaux les plus aimés étaient des lapins <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[l'orange est votre fruit préféré moins mais la fraise est son moins préféré <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[l'orange est votre fruit préféré moins mais la fraise est son moins préféré <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[les états unis est généralement agréable en juillet mais il est sec à l' automne <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[les états unis est généralement doux en juillet mais il est sec à l' automne <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[elle a conduit une brillante voiture noire <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[elle a conduit une voiture voiture noire <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[l' inde est parfois clémentes en septembre et il est parfois humide en hiver <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[l' inde est parfois clémentes en septembre et il est parfois humide en hiver <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[paris est beau au mois d' août mais il est agréable au printemps <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[paris est beau au mois d' août mais il est agréable au printemps <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      "BLEU-1: 0.970280\n",
      "BLEU-2: 0.957160\n",
      "BLEU-3: 0.949853\n",
      "BLEU-4: 0.931916\n",
      "TEST\n",
      " target=[californie est parfois sec au mois de mai et il est parfois merveilleux en février <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[californie est parfois doux au mois de mai et il est parfois merveilleux en février <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[californie est jamais chaud pendant l' automne mais il est jamais sec en mars <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[california est jamais chaud à l'automne mais il est jamais jamais sec mars <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[les états unis est parfois pluvieux en janvier mais il est doux en mai <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[les états unis est parfois pluvieux en janvier mais il est doux en mai <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[ils aiment les pommes les oranges et les poires <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[ils aiment les pommes les oranges et les poires <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[il n'aime les mangues et les fraises <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[il aime pas mangues mangues et les <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[le pamplemousse est son fruit préféré mais le citron est votre favori <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[le pamplemousse est son fruit préféré mais le citron est votre favori <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[l' inde est parfois agréable au cours de l' automne mais il est généralement agréable en mars <PAD> <PAD> <PAD> <PAD>], predicted=[l' inde est parfois agréable au cours de l' automne mais il est généralement agréable en mars <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[votre fruit le plus aimé est la pomme mais son plus aimé est la pêche <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[votre fruit le plus aimé est la pomme mais son plus aimé est la pêche <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[les états unis est froid pendant l' hiver et il est parfois relaxant en juin <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[les états unis est froid pendant l' hiver et il est parfois parfois en juin <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[i comme le pamplemousse les pommes et les mangues <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[i comme le pamplemousse les et et les mangues <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      "BLEU-1: 0.968703\n",
      "BLEU-2: 0.955058\n",
      "BLEU-3: 0.947505\n",
      "BLEU-4: 0.928880\n"
     ]
    }
   ],
   "source": [
    "# test on some training sequences\n",
    "print('TRAIN')\n",
    "pred1=MEGA_model.predict(X_train_embed)\n",
    "evaluate_model(MEGA_model , X_train_embed, y_train_embed,pred1)\n",
    "# test on some test sequences\n",
    "print('TEST')\n",
    "evaluate_model(MEGA_model, X_test_embed, y_test_embed,mega_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Si on essaye maintenant de traduire de nouvelles phrases (mais avec des mots vus à l'entrainement). Cela donne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=['he saw a old yellow truck',\n",
    "            'In France we like mangoes and we drive black cars.',\n",
    "           'the dogs dislike to eat bananas',\n",
    "           'the weather is midle during june and cooler in july',\n",
    "           'i want to visit Paris ' \n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(sentence,lenght=21):\n",
    "    for i in range(lenght-len(sentence)):\n",
    "        sentence.append(0)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_tokenizer.word_index['france']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 26 127 100 111 112 101   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0]\n",
      " [  2  24  97  92  74   7  97  93 102   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0]\n",
      " [  5 171  93  81 108  80   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0]\n",
      " [  5 193   1  64   4  34   7  57   2  43   0   0   0   0   0   0   0   0\n",
      "    0   0   0]\n",
      " [ 96 166  81 108  18   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "sentence = 'he saw a old yellow truck'\n",
    "sentence = [english_tokenizer.word_index[word] for word in sentence.split()]\n",
    "sentence = np.array(padding(sentence,lenght=21))\n",
    "\n",
    "sentence2 = 'in france we like mangoes and we dislike car'\n",
    "sentence2 = [english_tokenizer.word_index[word] for word in sentence2.split()]\n",
    "sentence2 = np.array(padding(sentence2,lenght=21))\n",
    "\n",
    "sentence3 = 'the dogs dislike to visit bananas'\n",
    "sentence3 = [english_tokenizer.word_index[word] for word in sentence3.split()]\n",
    "sentence3 = np.array(padding(sentence3,lenght=21))\n",
    "\n",
    "sentence4 = 'the weather is mild during june and cold in july'\n",
    "sentence4 = [english_tokenizer.word_index[word] for word in sentence4.split()]\n",
    "sentence4 = np.array(padding(sentence4,lenght=21))\n",
    "\n",
    "sentence5 = 'i want to visit paris '\n",
    "sentence5 = [english_tokenizer.word_index[word] for word in sentence5.split()]\n",
    "sentence5 = np.array(padding(sentence5,lenght=21))\n",
    "\n",
    "liste_a_tester=np.array([sentence,sentence2,sentence3,sentence4,sentence5])\n",
    "print(liste_a_tester)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pred=MEGA_model.predict(liste_a_tester)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION : il a vu un vieux camion jaune <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : he saw a old yellow truck\n",
      "PREDICTION : son animal le plus redouté est ce <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : In France we like mangoes and we drive black cars.\n",
      "PREDICTION : l' est est moins <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : the dogs dislike to eat bananas\n",
      "PREDICTION : le lion est est est est le <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : the weather is midle during june and cooler in july\n",
      "PREDICTION : il pourraient aller en californie <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : i want to visit Paris \n"
     ]
    }
   ],
   "source": [
    "for i in range(len(my_pred)):\n",
    "    print('PREDICTION :',logits_to_text(my_pred[i], french_tokenizer))\n",
    "    print('ATTENDU :',sentences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La prédiction sur de nouvelles phrases est pauvre, bien que le réseau connaisse déjà tous les mots employés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On essaye maintenant d'apprendre sur un plus gros jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    " # load datasets\n",
    "    \n",
    "dataset = load_clean_sentences('english-french-both.pkl')\n",
    "train = load_clean_sentences('english-french-train.pkl')\n",
    "test = load_clean_sentences('english-french-test.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_english_sentences=dataset[:,0]\n",
    "new_french_sentences=dataset[:,1]\n",
    "\n",
    "english_train=train[:,0]\n",
    "french_train=train[:,1]\n",
    "english_test=test[:,0]\n",
    "french_test=test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['i saw someone', 'what a pity', 'shake my hand', ..., 'be nice',\n",
       "       'see you around', 'hi'], dtype='<U339')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['je vis quelquun', 'quel malheur', 'serremoi la main', ...,\n",
       "       'sois gentil', 'au plaisir de vous revoir', 'salut'], dtype='<U339')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "french_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_preproc_english_sentences, new_preproc_french_sentences, new_english_tokenizer, new_french_tokenizer = preprocess(new_english_sentences, new_french_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1],\n",
       "       [326],\n",
       "       [212],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0]], dtype=int32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_preproc_french_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_english_tokenizer.word_index['saw'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2122"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_english_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed\n",
      "Max English sentence length: 5\n",
      "Max French sentence length: 10\n",
      "English vocabulary size: 2122\n",
      "French vocabulary size: 4373\n"
     ]
    }
   ],
   "source": [
    "new_max_english_sequence_length = new_preproc_english_sentences.shape[1]\n",
    "new_max_french_sequence_length = new_preproc_french_sentences.shape[1]\n",
    "new_english_vocab_size = len(new_english_tokenizer.word_index)\n",
    "new_french_vocab_size = len(new_french_tokenizer.word_index)\n",
    "\n",
    "print('Data Preprocessed')\n",
    "print(\"Max English sentence length:\", new_max_english_sequence_length)\n",
    "print(\"Max French sentence length:\", new_max_french_sequence_length)\n",
    "print(\"English vocabulary size:\", new_english_vocab_size)\n",
    "print(\"French vocabulary size:\", new_french_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_to_categorical(y_train):\n",
    "    res=np.zeros((y_train.shape[0],y_train.shape[1]))\n",
    "    for i in range(y_train.shape[0]):\n",
    "        for j in range(y_train.shape[1]):\n",
    "            res[i][j]=np.argmax(y_train[i][j])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tmp_x = pad(new_preproc_english_sentences, new_max_french_sequence_length)\n",
    "new_tmp_x = new_tmp_x.reshape((-1, new_preproc_french_sentences.shape[-2], 1))\n",
    "\n",
    "new_preproc_french_sentencesbis = ut.to_categorical(new_preproc_french_sentences,new_french_vocab_size+1)\n",
    "new_tmp_xbis = ut.to_categorical(new_tmp_x ) \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "new_X_train, new_X_test, new_y_train, new_y_test = train_test_split(new_tmp_xbis, new_preproc_french_sentencesbis, test_size=0.33, random_state=42)\n",
    "new_y_train_embed = new_y_train\n",
    "new_y_test_embed = new_y_test\n",
    "\n",
    "new_X_train_embed = inverse_to_categorical(new_X_train)\n",
    "new_X_test_embed = inverse_to_categorical(new_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6700, 10)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_X_train_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6700, 10, 2123)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6700, 10, 4374)\n",
      "(6700, 10, 4374)\n"
     ]
    }
   ],
   "source": [
    "print(new_y_train_embed.shape)\n",
    "print(new_y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3300, 10)\n",
      "(6700, 10)\n",
      "(3300, 10, 4374)\n",
      "(6700, 10, 4374)\n"
     ]
    }
   ],
   "source": [
    "print(new_X_test_embed.shape)\n",
    "print(new_X_train_embed.shape)\n",
    "print(new_y_test_embed.shape)\n",
    "print(new_y_train_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_13 (GRU)                 (None, 10, 512)           4048896   \n",
      "_________________________________________________________________\n",
      "time_distributed_13 (TimeDis (None, 10, 1024)          525312    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 10, 1024)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_14 (TimeDis (None, 10, 4374)          4483350   \n",
      "=================================================================\n",
      "Total params: 9,057,558\n",
      "Trainable params: 9,057,558\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 5360 samples, validate on 1340 samples\n",
      "Epoch 1/30\n",
      "5360/5360 [==============================] - 5s 853us/step - loss: 5.3671 - acc: 0.5518 - val_loss: 3.2376 - val_acc: 0.6789\n",
      "Epoch 2/30\n",
      "5360/5360 [==============================] - 2s 367us/step - loss: 4.2423 - acc: 0.5582 - val_loss: 2.9001 - val_acc: 0.6854\n",
      "Epoch 3/30\n",
      "5360/5360 [==============================] - 2s 361us/step - loss: 2.9934 - acc: 0.6664 - val_loss: 2.9826 - val_acc: 0.6895\n",
      "Epoch 4/30\n",
      "5360/5360 [==============================] - 2s 358us/step - loss: 2.8359 - acc: 0.6944 - val_loss: 2.7755 - val_acc: 0.6928\n",
      "Epoch 5/30\n",
      "5360/5360 [==============================] - 2s 361us/step - loss: 2.6185 - acc: 0.6923 - val_loss: 2.5063 - val_acc: 0.6933\n",
      "Epoch 6/30\n",
      "5360/5360 [==============================] - 2s 360us/step - loss: 2.3506 - acc: 0.6939 - val_loss: 2.3023 - val_acc: 0.7027\n",
      "Epoch 7/30\n",
      "5360/5360 [==============================] - 2s 362us/step - loss: 2.1529 - acc: 0.7041 - val_loss: 2.1650 - val_acc: 0.7148\n",
      "Epoch 8/30\n",
      "5360/5360 [==============================] - 2s 357us/step - loss: 1.9788 - acc: 0.7183 - val_loss: 2.0748 - val_acc: 0.7247\n",
      "Epoch 9/30\n",
      "5360/5360 [==============================] - 2s 356us/step - loss: 1.8560 - acc: 0.7285 - val_loss: 2.0163 - val_acc: 0.7310\n",
      "Epoch 10/30\n",
      "5360/5360 [==============================] - 2s 356us/step - loss: 1.7579 - acc: 0.7356 - val_loss: 1.9744 - val_acc: 0.7407\n",
      "Epoch 11/30\n",
      "5360/5360 [==============================] - 2s 363us/step - loss: 1.6727 - acc: 0.7432 - val_loss: 1.9397 - val_acc: 0.7446\n",
      "Epoch 12/30\n",
      "5360/5360 [==============================] - 2s 361us/step - loss: 1.5958 - acc: 0.7496 - val_loss: 1.9028 - val_acc: 0.7510\n",
      "Epoch 13/30\n",
      "5360/5360 [==============================] - 2s 363us/step - loss: 1.5231 - acc: 0.7551 - val_loss: 1.8733 - val_acc: 0.7596\n",
      "Epoch 14/30\n",
      "5360/5360 [==============================] - 2s 365us/step - loss: 1.4549 - acc: 0.7628 - val_loss: 1.8499 - val_acc: 0.7620\n",
      "Epoch 15/30\n",
      "5360/5360 [==============================] - 2s 361us/step - loss: 1.3890 - acc: 0.7669 - val_loss: 1.8300 - val_acc: 0.7643\n",
      "Epoch 16/30\n",
      "5360/5360 [==============================] - 2s 359us/step - loss: 1.3232 - acc: 0.7716 - val_loss: 1.8141 - val_acc: 0.7672\n",
      "Epoch 17/30\n",
      "5360/5360 [==============================] - 2s 358us/step - loss: 1.2590 - acc: 0.7772 - val_loss: 1.7982 - val_acc: 0.7708\n",
      "Epoch 18/30\n",
      "5360/5360 [==============================] - 2s 358us/step - loss: 1.1945 - acc: 0.7815 - val_loss: 1.7854 - val_acc: 0.7728\n",
      "Epoch 19/30\n",
      "5360/5360 [==============================] - 2s 365us/step - loss: 1.1340 - acc: 0.7870 - val_loss: 1.7737 - val_acc: 0.7757\n",
      "Epoch 20/30\n",
      "5360/5360 [==============================] - 2s 362us/step - loss: 1.0710 - acc: 0.7931 - val_loss: 1.7608 - val_acc: 0.7772\n",
      "Epoch 21/30\n",
      "5360/5360 [==============================] - 2s 365us/step - loss: 1.0105 - acc: 0.7980 - val_loss: 1.7487 - val_acc: 0.7760\n",
      "Epoch 22/30\n",
      "5360/5360 [==============================] - 2s 365us/step - loss: 0.9537 - acc: 0.8037 - val_loss: 1.7423 - val_acc: 0.7801\n",
      "Epoch 23/30\n",
      "5360/5360 [==============================] - 2s 365us/step - loss: 0.9050 - acc: 0.8113 - val_loss: 1.7357 - val_acc: 0.7795\n",
      "Epoch 24/30\n",
      "5360/5360 [==============================] - 2s 362us/step - loss: 0.8575 - acc: 0.8170 - val_loss: 1.7315 - val_acc: 0.7835\n",
      "Epoch 25/30\n",
      "5360/5360 [==============================] - 2s 362us/step - loss: 0.8169 - acc: 0.8237 - val_loss: 1.7209 - val_acc: 0.7833\n",
      "Epoch 26/30\n",
      "5360/5360 [==============================] - 2s 363us/step - loss: 0.7812 - acc: 0.8281 - val_loss: 1.7171 - val_acc: 0.7828\n",
      "Epoch 27/30\n",
      "5360/5360 [==============================] - 2s 354us/step - loss: 0.7470 - acc: 0.8326 - val_loss: 1.7169 - val_acc: 0.7841\n",
      "Epoch 28/30\n",
      "5360/5360 [==============================] - 2s 359us/step - loss: 0.7177 - acc: 0.8379 - val_loss: 1.7198 - val_acc: 0.7819\n",
      "Epoch 29/30\n",
      "5360/5360 [==============================] - 2s 365us/step - loss: 0.6913 - acc: 0.8412 - val_loss: 1.7226 - val_acc: 0.7831\n",
      "Epoch 30/30\n",
      "5360/5360 [==============================] - 2s 360us/step - loss: 0.6728 - acc: 0.8438 - val_loss: 1.7247 - val_acc: 0.7866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbb7c2fde80>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def new_simple_model(input_shape, output_sequence_length, new_english_vocab_size, new_french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a basic RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.005\n",
    "    \n",
    "    # TODO: Build the layers\n",
    "    model = Sequential()\n",
    "    model.add(GRU(512, input_shape=input_shape[1:], return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(new_french_vocab_size+1, activation='softmax'))) \n",
    "    \n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss=categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#tests.test_simple_model(simple_model)\n",
    "\n",
    "\n",
    "\n",
    "# Train the neural network\n",
    "new_simple_rnn_model = new_simple_model(\n",
    "    new_X_train.shape,\n",
    "    new_max_french_sequence_length,\n",
    "    new_english_vocab_size,\n",
    "    new_french_vocab_size)\n",
    "\n",
    "print(new_simple_rnn_model.summary())\n",
    "\n",
    "new_simple_rnn_model.fit(new_X_train,new_y_train, batch_size = 1000, epochs=30, validation_split=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_simple_pred= new_simple_rnn_model.predict(new_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION : grimpez <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : preparezvous <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : arrete <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : ca suffit tom <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : soyez le <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : sois realiste <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : je suis <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : je suis a la maison <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : aidemoi tom <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : aidez tom <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : detendstoi <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : calmezvous <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : ca a un <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : il a une fuite <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : etesvous <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : vous ennuyezvous <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : etesvous <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : tu es folle <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : je lai <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : jai improvise <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : je le le <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : je te trouverai <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : estce estelle est <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : estelle mariee <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : estce <PAD> votre <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : estce la tienne <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : ca va etre chaud <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : il va faire chaud <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : je le <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : je lattraperai <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : il travailla dur <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : il a travaille dur <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : tom a confiance confiance <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : tom vous fait confiance <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : je me suis <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : je me suis amuse <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : soyez satisfait <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : soyez satisfaits <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : je besoin besoin <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : jai besoin de vous <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(new_simple_pred[:20])):\n",
    "    print('PREDICTION :',logits_to_text(new_simple_pred[i], new_french_tokenizer))\n",
    "    print('ATTENDU :',logits_to_text(new_y_test[i], new_french_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_evaluate_model(model,  sources, raw_dataset,pred):\n",
    "    actual, predicted = list(), list()\n",
    "\n",
    "    for i in range(pred.shape[0]):\n",
    "        \n",
    "        # translate encoded source text\n",
    "        translation = logits_to_text(pred[i], new_french_tokenizer)\n",
    "        #raw_target=raw_dataset[i][0]\n",
    "        raw_target = logits_to_text(raw_dataset[i], new_french_tokenizer)\n",
    "        if i < 10:\n",
    "            print(' target=[%s], predicted=[%s]' % ( raw_target, translation))\n",
    "        actual.append([raw_target.split()])\n",
    "        predicted.append(translation.split())\n",
    "        \n",
    "    # calculate BLEU score\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pred1=new_simple_rnn_model.predict(new_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6700, 10, 4374)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_pred1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      " target=[quiconque estil dans la maison <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[quiconque estil a maison maison <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[cest sucre <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[cest un <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[jaime les chats <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[je les chats <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[vous etes invites <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[vous etes invitee <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[jaime les filles <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[je les filles <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[jaime le printemps <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[je les printemps <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[je voyage leger <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[je voyage leger <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[arrete de tinquieter <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[arretez de <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[je suis timide <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[je suis timide <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[qui a fait ca <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[qui a fait <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      "BLEU-1: 0.854537\n",
      "BLEU-2: 0.814629\n",
      "BLEU-3: 0.801855\n",
      "BLEU-4: 0.753007\n",
      "TEST\n",
      " target=[preparezvous <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[grimpez <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[ca suffit tom <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[arrete <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[sois realiste <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[soyez le <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[je suis a la maison <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[je suis <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[aidez tom <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[aidemoi tom <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[calmezvous <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[detendstoi <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[il a une fuite <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[ca a un <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[vous ennuyezvous <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[etesvous <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[tu es folle <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[etesvous <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[jai improvise <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[je lai <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      "BLEU-1: 0.798091\n",
      "BLEU-2: 0.745602\n",
      "BLEU-3: 0.729142\n",
      "BLEU-4: 0.667380\n"
     ]
    }
   ],
   "source": [
    "# test on some training sequences\n",
    "print('TRAIN')\n",
    "new_evaluate_model(new_simple_rnn_model , new_X_train, new_y_train,new_pred1)\n",
    "# test on some test sequences\n",
    "print('TEST')\n",
    "new_evaluate_model(new_simple_rnn_model, new_X_test, new_y_test,new_simple_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 10, 100)           212300    \n",
      "_________________________________________________________________\n",
      "gru_14 (GRU)                 (None, 10, 512)           941568    \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 512)               1181184   \n",
      "_________________________________________________________________\n",
      "repeat_vector_4 (RepeatVecto (None, 10, 512)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 10, 512)           1181184   \n",
      "_________________________________________________________________\n",
      "time_distributed_15 (TimeDis (None, 10, 1024)          525312    \n",
      "_________________________________________________________________\n",
      "gru_17 (GRU)                 (None, 10, 512)           2360832   \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 10, 512)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_16 (TimeDis (None, 10, 4374)          2243862   \n",
      "=================================================================\n",
      "Total params: 8,646,242\n",
      "Trainable params: 8,646,242\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5360 samples, validate on 1340 samples\n",
      "Epoch 1/60\n",
      "5360/5360 [==============================] - 16s 3ms/step - loss: 4.6292 - acc: 0.5518 - val_loss: 2.8140 - val_acc: 0.6789\n",
      "Epoch 2/60\n",
      "5360/5360 [==============================] - 2s 376us/step - loss: 2.6497 - acc: 0.6790 - val_loss: 2.5692 - val_acc: 0.6789\n",
      "Epoch 3/60\n",
      "5360/5360 [==============================] - 2s 347us/step - loss: 2.4241 - acc: 0.6862 - val_loss: 2.3885 - val_acc: 0.6996\n",
      "Epoch 4/60\n",
      "5360/5360 [==============================] - 2s 349us/step - loss: 2.2376 - acc: 0.6981 - val_loss: 2.3164 - val_acc: 0.6996\n",
      "Epoch 5/60\n",
      "5360/5360 [==============================] - 2s 343us/step - loss: 2.1611 - acc: 0.6982 - val_loss: 2.2561 - val_acc: 0.6996\n",
      "Epoch 6/60\n",
      "5360/5360 [==============================] - 2s 353us/step - loss: 2.1112 - acc: 0.6988 - val_loss: 2.2428 - val_acc: 0.7020\n",
      "Epoch 7/60\n",
      "5360/5360 [==============================] - 2s 345us/step - loss: 2.0880 - acc: 0.6993 - val_loss: 2.2327 - val_acc: 0.7020\n",
      "Epoch 8/60\n",
      "5360/5360 [==============================] - 2s 340us/step - loss: 2.0738 - acc: 0.6992 - val_loss: 2.2307 - val_acc: 0.7020\n",
      "Epoch 9/60\n",
      "5360/5360 [==============================] - 2s 342us/step - loss: 2.0595 - acc: 0.6996 - val_loss: 2.2296 - val_acc: 0.7020\n",
      "Epoch 10/60\n",
      "5360/5360 [==============================] - 2s 352us/step - loss: 2.0531 - acc: 0.6993 - val_loss: 2.2309 - val_acc: 0.7020\n",
      "Epoch 11/60\n",
      "5360/5360 [==============================] - 2s 356us/step - loss: 2.0459 - acc: 0.6995 - val_loss: 2.2286 - val_acc: 0.7020\n",
      "Epoch 12/60\n",
      "5360/5360 [==============================] - 2s 351us/step - loss: 2.0398 - acc: 0.6986 - val_loss: 2.2292 - val_acc: 0.7020\n",
      "Epoch 13/60\n",
      "5360/5360 [==============================] - 2s 352us/step - loss: 2.0360 - acc: 0.6996 - val_loss: 2.2312 - val_acc: 0.7020\n",
      "Epoch 14/60\n",
      "5360/5360 [==============================] - 2s 346us/step - loss: 2.0299 - acc: 0.6988 - val_loss: 2.2317 - val_acc: 0.7020\n",
      "Epoch 15/60\n",
      "5360/5360 [==============================] - 2s 352us/step - loss: 2.0295 - acc: 0.6996 - val_loss: 2.2372 - val_acc: 0.7020\n",
      "Epoch 16/60\n",
      "5360/5360 [==============================] - 2s 349us/step - loss: 2.0245 - acc: 0.6984 - val_loss: 2.2299 - val_acc: 0.7020\n",
      "Epoch 17/60\n",
      "5360/5360 [==============================] - 2s 351us/step - loss: 2.0177 - acc: 0.6994 - val_loss: 2.2284 - val_acc: 0.7020\n",
      "Epoch 18/60\n",
      "5360/5360 [==============================] - 2s 352us/step - loss: 2.0029 - acc: 0.6998 - val_loss: 2.2196 - val_acc: 0.7020\n",
      "Epoch 19/60\n",
      "5360/5360 [==============================] - 2s 347us/step - loss: 2.0224 - acc: 0.6993 - val_loss: 2.2287 - val_acc: 0.7020\n",
      "Epoch 20/60\n",
      "5360/5360 [==============================] - 2s 352us/step - loss: 2.0137 - acc: 0.6995 - val_loss: 2.2343 - val_acc: 0.7020\n",
      "Epoch 21/60\n",
      "5360/5360 [==============================] - 2s 354us/step - loss: 1.9983 - acc: 0.6996 - val_loss: 2.2305 - val_acc: 0.7030\n",
      "Epoch 22/60\n",
      "5360/5360 [==============================] - 2s 347us/step - loss: 1.9791 - acc: 0.6997 - val_loss: 2.1856 - val_acc: 0.7020\n",
      "Epoch 23/60\n",
      "5360/5360 [==============================] - 2s 348us/step - loss: 1.9478 - acc: 0.7028 - val_loss: 2.1579 - val_acc: 0.7096\n",
      "Epoch 24/60\n",
      "5360/5360 [==============================] - 2s 353us/step - loss: 1.9153 - acc: 0.7071 - val_loss: 2.1441 - val_acc: 0.7084\n",
      "Epoch 25/60\n",
      "5360/5360 [==============================] - 2s 350us/step - loss: 1.8889 - acc: 0.7088 - val_loss: 2.1254 - val_acc: 0.7106\n",
      "Epoch 26/60\n",
      "5360/5360 [==============================] - 2s 350us/step - loss: 1.8731 - acc: 0.7096 - val_loss: 2.1391 - val_acc: 0.7103\n",
      "Epoch 27/60\n",
      "5360/5360 [==============================] - 2s 354us/step - loss: 1.8616 - acc: 0.7101 - val_loss: 2.1018 - val_acc: 0.7137\n",
      "Epoch 28/60\n",
      "5360/5360 [==============================] - 2s 350us/step - loss: 1.8288 - acc: 0.7123 - val_loss: 2.1641 - val_acc: 0.7033\n",
      "Epoch 29/60\n",
      "5360/5360 [==============================] - 2s 349us/step - loss: 1.8489 - acc: 0.7101 - val_loss: 2.1016 - val_acc: 0.7141\n",
      "Epoch 30/60\n",
      "5360/5360 [==============================] - 2s 351us/step - loss: 1.8058 - acc: 0.7155 - val_loss: 2.1054 - val_acc: 0.7128\n",
      "Epoch 31/60\n",
      "5360/5360 [==============================] - 2s 353us/step - loss: 1.7973 - acc: 0.7151 - val_loss: 2.0841 - val_acc: 0.7160\n",
      "Epoch 32/60\n",
      "5360/5360 [==============================] - 2s 348us/step - loss: 1.7691 - acc: 0.7171 - val_loss: 2.0848 - val_acc: 0.7169\n",
      "Epoch 33/60\n",
      "5360/5360 [==============================] - 2s 350us/step - loss: 1.7612 - acc: 0.7181 - val_loss: 2.0793 - val_acc: 0.7187\n",
      "Epoch 34/60\n",
      "5360/5360 [==============================] - 2s 349us/step - loss: 1.7558 - acc: 0.7191 - val_loss: 2.0897 - val_acc: 0.7189\n",
      "Epoch 35/60\n",
      "5360/5360 [==============================] - 2s 354us/step - loss: 1.7432 - acc: 0.7204 - val_loss: 2.1120 - val_acc: 0.7166\n",
      "Epoch 36/60\n",
      "5360/5360 [==============================] - 2s 354us/step - loss: 1.7485 - acc: 0.7193 - val_loss: 2.0978 - val_acc: 0.7169\n",
      "Epoch 37/60\n",
      "5360/5360 [==============================] - 2s 346us/step - loss: 1.7434 - acc: 0.7188 - val_loss: 2.1138 - val_acc: 0.7116\n",
      "Epoch 38/60\n",
      "5360/5360 [==============================] - 2s 354us/step - loss: 1.7358 - acc: 0.7183 - val_loss: 2.0749 - val_acc: 0.7212\n",
      "Epoch 39/60\n",
      "5360/5360 [==============================] - 2s 352us/step - loss: 1.7184 - acc: 0.7205 - val_loss: 2.0888 - val_acc: 0.7213\n",
      "Epoch 40/60\n",
      "5360/5360 [==============================] - 2s 350us/step - loss: 1.7115 - acc: 0.7223 - val_loss: 2.0752 - val_acc: 0.7220\n",
      "Epoch 41/60\n",
      "5360/5360 [==============================] - 2s 351us/step - loss: 1.6984 - acc: 0.7222 - val_loss: 2.0788 - val_acc: 0.7235\n",
      "Epoch 42/60\n",
      "5360/5360 [==============================] - 2s 355us/step - loss: 1.6896 - acc: 0.7226 - val_loss: 2.0730 - val_acc: 0.7221\n",
      "Epoch 43/60\n",
      "5360/5360 [==============================] - 2s 355us/step - loss: 1.7018 - acc: 0.7219 - val_loss: 2.0892 - val_acc: 0.7223\n",
      "Epoch 44/60\n",
      "5360/5360 [==============================] - 2s 358us/step - loss: 1.6984 - acc: 0.7225 - val_loss: 2.0713 - val_acc: 0.7222\n",
      "Epoch 45/60\n",
      "5360/5360 [==============================] - 2s 351us/step - loss: 1.6682 - acc: 0.7231 - val_loss: 2.0619 - val_acc: 0.7234\n",
      "Epoch 46/60\n",
      "5360/5360 [==============================] - 2s 352us/step - loss: 1.6594 - acc: 0.7239 - val_loss: 2.0626 - val_acc: 0.7254\n",
      "Epoch 47/60\n",
      "5360/5360 [==============================] - 2s 351us/step - loss: 1.6378 - acc: 0.7260 - val_loss: 2.0429 - val_acc: 0.7249\n",
      "Epoch 48/60\n",
      "5360/5360 [==============================] - 2s 353us/step - loss: 1.6267 - acc: 0.7259 - val_loss: 2.0606 - val_acc: 0.7235\n",
      "Epoch 49/60\n",
      "5360/5360 [==============================] - 2s 353us/step - loss: 1.6930 - acc: 0.7245 - val_loss: 2.0884 - val_acc: 0.7246\n",
      "Epoch 50/60\n",
      "5360/5360 [==============================] - 2s 357us/step - loss: 1.6889 - acc: 0.7226 - val_loss: 2.0805 - val_acc: 0.7190\n",
      "Epoch 51/60\n",
      "5360/5360 [==============================] - 2s 355us/step - loss: 1.6625 - acc: 0.7213 - val_loss: 2.0455 - val_acc: 0.7260\n",
      "Epoch 52/60\n",
      "5360/5360 [==============================] - 2s 348us/step - loss: 1.6347 - acc: 0.7229 - val_loss: 2.0481 - val_acc: 0.7227\n",
      "Epoch 53/60\n",
      "5360/5360 [==============================] - 2s 356us/step - loss: 1.6184 - acc: 0.7240 - val_loss: 2.0338 - val_acc: 0.7239\n",
      "Epoch 54/60\n",
      "5360/5360 [==============================] - 2s 349us/step - loss: 1.5942 - acc: 0.7250 - val_loss: 2.0255 - val_acc: 0.7266\n",
      "Epoch 55/60\n",
      "5360/5360 [==============================] - 2s 355us/step - loss: 1.5750 - acc: 0.7276 - val_loss: 2.0216 - val_acc: 0.7299\n",
      "Epoch 56/60\n",
      "5360/5360 [==============================] - 2s 355us/step - loss: 1.5683 - acc: 0.7291 - val_loss: 2.0110 - val_acc: 0.7303\n",
      "Epoch 57/60\n",
      "5360/5360 [==============================] - 2s 351us/step - loss: 1.5469 - acc: 0.7307 - val_loss: 1.9997 - val_acc: 0.7317\n",
      "Epoch 58/60\n",
      "5360/5360 [==============================] - 2s 352us/step - loss: 1.5477 - acc: 0.7313 - val_loss: 1.9976 - val_acc: 0.7304\n",
      "Epoch 59/60\n",
      "5360/5360 [==============================] - 2s 351us/step - loss: 1.5991 - acc: 0.7266 - val_loss: 2.0443 - val_acc: 0.7263\n",
      "Epoch 60/60\n",
      "5360/5360 [==============================] - 2s 355us/step - loss: 1.5791 - acc: 0.7265 - val_loss: 2.0008 - val_acc: 0.7270\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbb7a948278>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def new_mega_combo_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps,input_shape):\n",
    "    \"\"\"\n",
    "    Build and train a RNN model using word embedding on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(src_vocab,100, mask_zero = False, input_shape=input_shape[1:]))\n",
    "    model.add(GRU(512,return_sequences=True))  \n",
    "    model.add(Bidirectional(GRU(256)))\n",
    "    model.add(RepeatVector(10))\n",
    "    model.add(Bidirectional(GRU(256, return_sequences=True)))\n",
    "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
    "    model.add(GRU(512,return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(new_french_vocab_size+1, activation='softmax')))\n",
    "\n",
    "    return model\n",
    "\n",
    "learning_rate = 0.005\n",
    "new_MEGA_model = new_mega_combo_model(\n",
    "    len(new_english_tokenizer.word_index)+1,\n",
    "    len(new_french_tokenizer.word_index)+1,\n",
    "    new_y_train.shape[1:],\n",
    "    10,\n",
    "    new_X_train_embed.shape)\n",
    "\n",
    "new_MEGA_model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "new_MEGA_model.summary()\n",
    "\n",
    "new_MEGA_model.fit(new_X_train_embed, new_y_train_embed, batch_size=1000, epochs=60, validation_split=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mega_pred = new_MEGA_model.predict(new_X_test_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION : soyez <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : preparezvous <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : venez a <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : ca suffit tom <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : soyez prudente <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : sois realiste <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : je suis en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : je suis a la maison <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : soyez <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : aidez tom <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : oubliezle <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : calmezvous <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : cest est <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : il a une fuite <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : etesvous <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : vous ennuyezvous <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : etesvous <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : tu es folle <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : je suis <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : jai improvise <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : je sommes <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : je te trouverai <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : cest un <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : estelle mariee <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : cest <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : estce la tienne <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : cest un <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : il va faire chaud <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : je sommes <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : je lattraperai <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : tom est <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : il a travaille dur <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : je ne suis pas <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : tom vous fait confiance <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : je ne pas pas <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : je me suis amuse <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : soyez prudente <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : soyez satisfaits <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : je ne suis pas <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : jai besoin de vous <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(new_mega_pred[:20])):\n",
    "    print('PREDICTION :',logits_to_text(new_mega_pred[i], new_french_tokenizer))\n",
    "    print('ATTENDU :',logits_to_text(new_y_test[i], new_french_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mega_pred2 = new_MEGA_model.predict(new_X_train_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      " target=[quiconque estil dans la maison <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[je ne suis pas <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[cest sucre <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[cest un <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[jaime les chats <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[ils sont <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[vous etes invites <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[cest es <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[jaime les filles <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[jaime sont <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[jaime le printemps <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[jaime sont <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[je voyage leger <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[je sommes <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[arrete de tinquieter <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[arrete de <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[je suis timide <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[je suis <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[qui a fait ca <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[tom est <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      "BLEU-1: 0.732463\n",
      "BLEU-2: 0.692048\n",
      "BLEU-3: 0.687499\n",
      "BLEU-4: 0.627348\n",
      "TEST\n",
      " target=[preparezvous <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[soyez <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[ca suffit tom <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[venez a <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[sois realiste <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[soyez prudente <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[je suis a la maison <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[je suis en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[aidez tom <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[soyez <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[calmezvous <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[oubliezle <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[il a une fuite <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[cest est <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[vous ennuyezvous <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[etesvous <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[tu es folle <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[etesvous <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[jai improvise <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[je suis <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      "BLEU-1: 0.730212\n",
      "BLEU-2: 0.690419\n",
      "BLEU-3: 0.686071\n",
      "BLEU-4: 0.625936\n"
     ]
    }
   ],
   "source": [
    "# test on some training sequences\n",
    "print('TRAIN')\n",
    "new_evaluate_model(new_MEGA_model , new_X_train_embed, new_y_train_embed,new_mega_pred2)\n",
    "# test on some test sequences\n",
    "print('TEST')\n",
    "new_evaluate_model(new_MEGA_model, new_X_test_embed, new_y_test_embed,new_mega_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_13 (GRU)                 (None, 10, 512)           4048896   \n",
      "_________________________________________________________________\n",
      "time_distributed_13 (TimeDis (None, 10, 1024)          525312    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 10, 1024)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_14 (TimeDis (None, 10, 4374)          4483350   \n",
      "=================================================================\n",
      "Total params: 9,057,558\n",
      "Trainable params: 9,057,558\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 5360 samples, validate on 1340 samples\n",
      "Epoch 1/30\n",
      "5360/5360 [==============================] - 10s 2ms/step - loss: 5.3013 - acc: 0.5536 - val_loss: 3.2641 - val_acc: 0.6789\n",
      "Epoch 2/30\n",
      "5360/5360 [==============================] - 2s 353us/step - loss: 3.2316 - acc: 0.6531 - val_loss: 2.9262 - val_acc: 0.6824\n",
      "Epoch 3/30\n",
      "5360/5360 [==============================] - 2s 354us/step - loss: 3.0615 - acc: 0.6619 - val_loss: 2.9825 - val_acc: 0.6796\n",
      "Epoch 4/30\n",
      "5360/5360 [==============================] - 2s 365us/step - loss: 2.8272 - acc: 0.6833 - val_loss: 2.7158 - val_acc: 0.6859\n",
      "Epoch 5/30\n",
      "5360/5360 [==============================] - 2s 357us/step - loss: 2.5537 - acc: 0.6854 - val_loss: 2.4348 - val_acc: 0.6912\n",
      "Epoch 6/30\n",
      "5360/5360 [==============================] - 2s 360us/step - loss: 2.2855 - acc: 0.6938 - val_loss: 2.2633 - val_acc: 0.6978\n",
      "Epoch 7/30\n",
      "5360/5360 [==============================] - 2s 358us/step - loss: 2.0724 - acc: 0.7053 - val_loss: 2.1317 - val_acc: 0.7153\n",
      "Epoch 8/30\n",
      "5360/5360 [==============================] - 2s 356us/step - loss: 1.9233 - acc: 0.7191 - val_loss: 2.0517 - val_acc: 0.7324\n",
      "Epoch 9/30\n",
      "5360/5360 [==============================] - 2s 356us/step - loss: 1.8068 - acc: 0.7307 - val_loss: 1.9879 - val_acc: 0.7351\n",
      "Epoch 10/30\n",
      "5360/5360 [==============================] - 2s 349us/step - loss: 1.7107 - acc: 0.7393 - val_loss: 1.9389 - val_acc: 0.7472\n",
      "Epoch 11/30\n",
      "5360/5360 [==============================] - 2s 358us/step - loss: 1.6260 - acc: 0.7499 - val_loss: 1.9059 - val_acc: 0.7501\n",
      "Epoch 12/30\n",
      "5360/5360 [==============================] - 2s 357us/step - loss: 1.5540 - acc: 0.7535 - val_loss: 1.8810 - val_acc: 0.7560\n",
      "Epoch 13/30\n",
      "5360/5360 [==============================] - 2s 355us/step - loss: 1.4855 - acc: 0.7595 - val_loss: 1.8595 - val_acc: 0.7584\n",
      "Epoch 14/30\n",
      "5360/5360 [==============================] - 2s 352us/step - loss: 1.4187 - acc: 0.7643 - val_loss: 1.8365 - val_acc: 0.7611\n",
      "Epoch 15/30\n",
      "5360/5360 [==============================] - 2s 360us/step - loss: 1.3532 - acc: 0.7679 - val_loss: 1.8163 - val_acc: 0.7632\n",
      "Epoch 16/30\n",
      "5360/5360 [==============================] - 2s 356us/step - loss: 1.2858 - acc: 0.7735 - val_loss: 1.8029 - val_acc: 0.7668\n",
      "Epoch 17/30\n",
      "5360/5360 [==============================] - 2s 354us/step - loss: 1.2210 - acc: 0.7794 - val_loss: 1.7891 - val_acc: 0.7697\n",
      "Epoch 18/30\n",
      "5360/5360 [==============================] - 2s 355us/step - loss: 1.1551 - acc: 0.7840 - val_loss: 1.7738 - val_acc: 0.7717\n",
      "Epoch 19/30\n",
      "5360/5360 [==============================] - 2s 356us/step - loss: 1.0876 - acc: 0.7897 - val_loss: 1.7632 - val_acc: 0.7734\n",
      "Epoch 20/30\n",
      "5360/5360 [==============================] - 2s 354us/step - loss: 1.0250 - acc: 0.7960 - val_loss: 1.7505 - val_acc: 0.7772\n",
      "Epoch 21/30\n",
      "5360/5360 [==============================] - 2s 358us/step - loss: 0.9607 - acc: 0.8027 - val_loss: 1.7375 - val_acc: 0.7770\n",
      "Epoch 22/30\n",
      "5360/5360 [==============================] - 2s 354us/step - loss: 0.9060 - acc: 0.8099 - val_loss: 1.7237 - val_acc: 0.7781\n",
      "Epoch 23/30\n",
      "5360/5360 [==============================] - 2s 356us/step - loss: 0.8503 - acc: 0.8184 - val_loss: 1.7262 - val_acc: 0.7816\n",
      "Epoch 24/30\n",
      "5360/5360 [==============================] - 2s 360us/step - loss: 0.8104 - acc: 0.8217 - val_loss: 1.7155 - val_acc: 0.7811\n",
      "Epoch 25/30\n",
      "5360/5360 [==============================] - 2s 355us/step - loss: 0.7610 - acc: 0.8299 - val_loss: 1.7104 - val_acc: 0.7808\n",
      "Epoch 26/30\n",
      "5360/5360 [==============================] - 2s 357us/step - loss: 0.7261 - acc: 0.8330 - val_loss: 1.7113 - val_acc: 0.7813\n",
      "Epoch 27/30\n",
      "5360/5360 [==============================] - 2s 352us/step - loss: 0.6958 - acc: 0.8395 - val_loss: 1.7057 - val_acc: 0.7828\n",
      "Epoch 28/30\n",
      "5360/5360 [==============================] - 2s 354us/step - loss: 0.6681 - acc: 0.8431 - val_loss: 1.7031 - val_acc: 0.7837\n",
      "Epoch 29/30\n",
      "5360/5360 [==============================] - 2s 360us/step - loss: 0.6404 - acc: 0.8490 - val_loss: 1.7071 - val_acc: 0.7829\n",
      "Epoch 30/30\n",
      "5360/5360 [==============================] - 2s 354us/step - loss: 0.6180 - acc: 0.8521 - val_loss: 1.7095 - val_acc: 0.7833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbb7486f4e0>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def new_simple_model_lstm (input_shape, output_sequence_length, new_english_vocab_size, new_french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a basic RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.005\n",
    "    \n",
    "    # TODO: Build the layers\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, input_shape=input_shape[1:], return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(new_french_vocab_size+1, activation='softmax'))) \n",
    "    \n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss=categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#tests.test_simple_model(simple_model)\n",
    "\n",
    "\n",
    "\n",
    "# Train the neural network\n",
    "new_simple_rnn_model_lstm = new_simple_model(\n",
    "    new_X_train.shape,\n",
    "    new_max_french_sequence_length,\n",
    "    new_english_vocab_size,\n",
    "    new_french_vocab_size)\n",
    "\n",
    "print(new_simple_rnn_model.summary())\n",
    "\n",
    "new_simple_rnn_model_lstm.fit(new_X_train,new_y_train, batch_size = 1000, epochs=30, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_simple_pred_lstm = new_simple_rnn_model_lstm.predict(new_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION : grimpez <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : preparezvous <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : arrete <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : ca suffit tom <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : soyez le <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : sois realiste <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : je suis <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : je suis a la maison <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : aidemoi tom <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : aidez tom <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : detendstoi <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : calmezvous <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : ca a un <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : il a une fuite <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : etesvous <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : vous ennuyezvous <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : etesvous <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : tu es folle <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : je lai <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : jai improvise <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : je le le <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : je te trouverai <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : estce estelle est <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : estelle mariee <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : estce <PAD> votre <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : estce la tienne <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : ca va etre chaud <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : il va faire chaud <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : je le <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : je lattraperai <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : il travailla dur <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : il a travaille dur <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : tom a confiance confiance <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : tom vous fait confiance <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : je me suis <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : je me suis amuse <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : soyez satisfait <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : soyez satisfaits <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "PREDICTION : je besoin besoin <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : jai besoin de vous <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(new_simple_pred_lstm[:20])):\n",
    "    print('PREDICTION :',logits_to_text(new_simple_pred_lstm[i], new_french_tokenizer))\n",
    "    print('ATTENDU :',logits_to_text(new_y_test[i], new_french_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_simple_pred_lstm2 = new_simple_rnn_model_lstm.predict(new_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      " target=[quiconque estil dans la maison <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[quiconque estil a la maison <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[cest sucre <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[cest un <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[jaime les chats <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[je les chats <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[vous etes invites <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[vous etes invitee <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[jaime les filles <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[je les filles <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[jaime le printemps <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[je les printemps <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[je voyage leger <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[je voyage leger <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[arrete de tinquieter <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[arrete de tinquieter <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[je suis timide <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[je suis timide <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[qui a fait ca <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[qui a fait <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      "BLEU-1: 0.859522\n",
      "BLEU-2: 0.822158\n",
      "BLEU-3: 0.810476\n",
      "BLEU-4: 0.764161\n",
      "TEST\n",
      " target=[preparezvous <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[allez <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[ca suffit tom <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[arretez <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[sois realiste <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[soyez <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[je suis a la maison <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[je suis <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[aidez tom <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[aidenous tom <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[calmezvous <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[detendstoi <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[il a une fuite <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[ca a un <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[vous ennuyezvous <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[etesvous <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[tu es folle <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[etesvous <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      " target=[jai improvise <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>], predicted=[je me <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>]\n",
      "BLEU-1: 0.797303\n",
      "BLEU-2: 0.746134\n",
      "BLEU-3: 0.729705\n",
      "BLEU-4: 0.667784\n"
     ]
    }
   ],
   "source": [
    "# test on some training sequences\n",
    "print('TRAIN')\n",
    "new_evaluate_model(new_simple_pred_lstm, new_X_train, new_y_train , new_simple_pred_lstm2 )\n",
    "# test on some test sequences\n",
    "print('TEST')\n",
    "new_evaluate_model(new_simple_pred_lstm, new_X_test, new_y_test , new_simple_pred_lstm )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  12   78    5  135 1996 1769    0    0    0    0]\n",
      " [  42   36   58    8   40   22   92  433  713 1030]\n",
      " [  24  303 1279   22  111 1636    0    0    0    0]\n",
      " [ 134  693 1076  128    0    0    0    0    0    0]\n",
      " [  75  126   10 1275  711    2   21   25   87    0]]\n"
     ]
    }
   ],
   "source": [
    "new_sentence = 'he saw a old yellow truck'\n",
    "new_sentence_test = [new_english_tokenizer.word_index[word] for word in new_sentence.split()]\n",
    "new_sentence_test = np.array(padding(new_sentence_test,lenght=10))\n",
    "\n",
    "new_sentence2 = 'in my home we like to work and red cars'\n",
    "new_sentence2_test = [new_english_tokenizer.word_index[word] for word in new_sentence2.split()]\n",
    "new_sentence2_test = np.array(padding(new_sentence2_test,lenght=10))\n",
    "\n",
    "new_sentence3 = 'the dogs dislike to eat bananas'\n",
    "new_sentence3_test = [new_english_tokenizer.word_index[word] for word in new_sentence3.split()]\n",
    "new_sentence3_test = np.array(padding(new_sentence3_test,lenght=10))\n",
    "\n",
    "new_sentence4 = 'say hi next time'\n",
    "new_sentence4_test = [new_english_tokenizer.word_index[word] for word in new_sentence4.split()]\n",
    "new_sentence4_test = np.array(padding(new_sentence4_test,lenght=10))\n",
    "\n",
    "new_sentence5 = \"please ask me something if you do not know\"\n",
    "new_sentence5_test = [new_english_tokenizer.word_index[word] for word in new_sentence5.split()]\n",
    "new_sentence5_test = np.array(padding(new_sentence5_test,lenght=10))\n",
    "\n",
    "new_liste_a_tester=np.array([new_sentence_test,new_sentence2_test,new_sentence3_test,new_sentence4_test,new_sentence5_test])\n",
    "new_sentences=[new_sentence,new_sentence2,new_sentence3,new_sentence4,new_sentence5]\n",
    "print(new_liste_a_tester)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION : je ne suis <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : he saw a old yellow truck\n",
      "PREDICTION : je suis <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : in my home we like to work and red cars\n",
      "PREDICTION : je ne suis pas <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : the dogs dislike to eat bananas\n",
      "PREDICTION : puisje de <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : say hi next time\n",
      "PREDICTION : ne ne pas pas <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : please ask me something if you do not know\n"
     ]
    }
   ],
   "source": [
    "new_my_pred=new_MEGA_model.predict(new_liste_a_tester)\n",
    "for i in range(len(new_my_pred)):\n",
    "    print('PREDICTION :',logits_to_text(new_my_pred[i], new_french_tokenizer))\n",
    "    print('ATTENDU :',new_sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_new_liste_a_tester=ut.to_categorical(new_liste_a_tester,2123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION : il vu un un vieux <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : he saw a old yellow truck\n",
      "PREDICTION : tom la la la la <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : in my home we like to work and red cars\n",
      "PREDICTION : la les chiens les les <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : the dogs dislike to eat bananas\n",
      "PREDICTION : ditesle non <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : say hi next time\n",
      "PREDICTION : sil vous sil lacet lacet prie <PAD> <PAD> <PAD> <PAD>\n",
      "ATTENDU : please ask me something if you do not know\n"
     ]
    }
   ],
   "source": [
    "new_my_pred=new_simple_rnn_model.predict(simple_new_liste_a_tester)\n",
    "for i in range(len(new_my_pred)):\n",
    "    print('PREDICTION :',logits_to_text(new_my_pred[i], new_french_tokenizer))\n",
    "    print('ATTENDU :',new_sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous voulions importer une méthode WordtoVec pré-entraînée mais les formats de ces grosses bibliothèques sont difficiles à télécharger et à décompresser... Mais une idée pour continuer ce projet serait d'essayer une fois les mots prétraités avec la méthode WordToVec de prédire des nouvelles phrases avec des mots que le réseau n'a jamais vu lors de son entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/GoogleGoogleNews-vectors-negative300.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-5f859f19f665>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Load vectors directly from the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/GoogleGoogleNews-vectors-negative300.bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# Access vectors for specific words with a keyed lookup:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'easy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/insa/anaconda/envs/GPU/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1496\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1497\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1498\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/insa/anaconda/envs/GPU/lib/python3.7/site-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# throws for invalid file format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/insa/anaconda/envs/GPU/lib/python3.7/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, transport_params)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m     )\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/insa/anaconda/envs/GPU/lib/python3.7/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, ignore_ext, buffering, encoding, errors)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPY3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/GoogleGoogleNews-vectors-negative300.bin'"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "# Load vectors directly from the file\n",
    "model = KeyedVectors.load_word2vec_format('data/GoogleGoogleNews-vectors-negative300.bin', binary=True)\n",
    "# Access vectors for specific words with a keyed lookup:\n",
    "vector = model['easy']\n",
    "# see the shape of the vector (300,)\n",
    "vector.shape\n",
    "# Processing sentences is not as simple as with Spacy:\n",
    "vectors = [model[x] for x in \"This is some text I am processing with Spacy\".split(' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
